[
  {
    "path": "posts/2021-10-03-nfl-scorelines/",
    "title": "NFL Scorelines",
    "description": "Which NFL scores have never happened?",
    "author": [
      {
        "name": "James Curley",
        "url": "jamescurley.blog"
      }
    ],
    "date": "2021-10-03",
    "categories": [
      "nfl",
      "scraping"
    ],
    "contents": "\r\nThere have been over 16,860 games of professional American Football. There have been some rule changes to the points scoring system over that time, but clearly some scores are going to be more likely than others. As a touchdown plus extra point is worth 7, we’d expect a higher amount of games with multiples of 7 in the scoreline. Similarly, we’d expect more scorelines with multiples of 3 (the number of extra points).\r\nGiven the large number of games already played, we would expect that it would be rare for unique, never happened before, scorelines to occur. In fact, there is a fun website called Scorigami that tracks the likelihood of this happening for each ongoing NFL game.\r\nThere is also a nice neat list of the number of times each pro scoreline has occurred at the Pro Football Reference website. One important thing to note is that this list does not distinguish between home/road teams. So, for example a 17-20 road win is considered to be the same as a 20-17 home win.\r\nIn this post, I thought it would be fun to visualize these scorelines and how often they occurred.\r\n\r\nGetting the Data\r\nThe first step is to scrape the data. We will do that using the rvest package:\r\n\r\n\r\nlibrary(rvest)\r\nlibrary(tidyverse)\r\n\r\nurl <- \"https://www.pro-football-reference.com/boxscores/game-scores.htm\"\r\n\r\ntab <- url %>%\r\n  read_html(url) %>%\r\n html_node(\"table\") %>%\r\n html_table()\r\n\r\nhead(tab)\r\n\r\n\r\n# A tibble: 6 x 9\r\n     Rk Score  PtsW  PtsL PtTot    PD Count ``        `Last Game`     \r\n  <int> <chr> <int> <int> <int> <int> <int> <chr>     <chr>           \r\n1     1 20-17    20    17    37     3   271 all games Chicago Bears v~\r\n2     2 27-24    27    24    51     3   219 all games Los Angeles Ram~\r\n3     3 17-14    17    14    31     3   197 all games Atlanta Falcons~\r\n4     4 23-20    23    20    43     3   189 all games New York Jets v~\r\n5     5 24-17    24    17    41     7   169 all games Tennessee Titan~\r\n6     6 13-10    13    10    23     3   163 all games Detroit Lions v~\r\n\r\ntail(tab)\r\n\r\n\r\n# A tibble: 6 x 9\r\n     Rk Score  PtsW  PtsL PtTot    PD Count ``        `Last Game`     \r\n  <int> <chr> <int> <int> <int> <int> <int> <chr>     <chr>           \r\n1  1061 70-27    70    27    97    43     1 all games Los Angeles Ram~\r\n2  1062 42-38    42    38    80     4     1 all games Los Angeles Ram~\r\n3  1063 11-6     11     6    17     5     1 all games St. Louis Rams ~\r\n4  1064 46-9     46     9    55    37     1 all games Los Angeles Ram~\r\n5  1065 66-0     66     0    66    66     1 all games Rochester Jeffe~\r\n6  1066 50-28    50    28    78    22     1 all games San Diego Charg~\r\n\r\n\r\nWe could tidy these data up, but they already contain the three columns that we need. The PtsW and PtsL columns contain the scores for each team. The Count column contains the number of times each scoreline occurred.\r\nWhat we can do is plot these data as a matrix using geom_tile() and filling the color of each tile based on the count variable.\r\nWe also need to know the range of possible scores to limit the axes. These are:\r\n\r\n\r\nrange(tab$PtsW)\r\n\r\n\r\n[1]  0 73\r\n\r\nrange(tab$PtsL)\r\n\r\n\r\n[1]  0 51\r\n\r\n\r\nThis is the plot - you may need to zoom in to check each score, but they should be visible:\r\n\r\n\r\nggplot(tab, aes(x=PtsW, y=PtsL, fill=sqrt(Count))) +\r\n  geom_tile(color='black') +\r\n  geom_text(aes(label=Count), size=1.8, color='black')+\r\n  scale_fill_continuous(low=\"#FFF973\", high=\"#F92A0D\") +\r\n  scale_y_reverse(breaks=seq(0,52,2)) +\r\n  scale_x_continuous(breaks=seq(0,74,2),position = \"top\")+\r\n  xlab(\"Winning Team Points\") +\r\n  ylab(\"Losing Team Points\") +\r\n  theme(\r\n    plot.background = element_rect(fill=\"white\"),\r\n    panel.background = element_rect(fill=\"white\"),\r\n    panel.border = element_rect(fill=NA, color=\"white\", size=0.15, linetype=\"solid\"),\r\n    axis.text = element_text(color=\"black\", size=rel(0.7)),\r\n    axis.text.y  = element_text(hjust=1),\r\n    legend.position = \"none\"\r\n     )\r\n\r\n\r\n\r\n\r\n\r\nI chose to make the tick marks on the x and y axes separated by 2. I think this still makes the chart readable while not making it too clustered. Perhaps one issue with the above graph is that it isn’t that easy to follow rows and columns as not the tiles with missing values do not have borders.\r\nIf we had borders it might be easier to see which scores have yet to occur. To make a plot with borders around each tile, we need to include every combination of values in the data.frame. I did this by joining the tab data.frame with one that contained each winning and losing possible score, and a 0 in the Count column, then summing across each win/loss score, and then making the zeros into NA so that they would appear white in the final plot.\r\n\r\n\r\neg <- expand.grid(0:74,0:52)\r\n\r\ndf <- \r\n  data.frame(\r\n  PtsW = eg[,1],\r\n  PtsL = eg[,2],\r\n  Count=0\r\n) \r\n\r\ndf <- \r\n  full_join(df, tab %>% select(PtsW, PtsL, Count)) %>%\r\n  group_by(PtsW,PtsL) %>%\r\n  summarise(Count=sum(Count))\r\n\r\nhead(df)\r\n\r\n\r\n# A tibble: 6 x 3\r\n# Groups:   PtsW [1]\r\n   PtsW  PtsL Count\r\n  <int> <int> <dbl>\r\n1     0     0    73\r\n2     0     1     0\r\n3     0     2     0\r\n4     0     3     0\r\n5     0     4     0\r\n6     0     5     0\r\n\r\ndf$Count <- ifelse(df$Count==0,NA,df$Count)\r\n\r\nhead(df)\r\n\r\n\r\n# A tibble: 6 x 3\r\n# Groups:   PtsW [1]\r\n   PtsW  PtsL Count\r\n  <int> <int> <dbl>\r\n1     0     0    73\r\n2     0     1    NA\r\n3     0     2    NA\r\n4     0     3    NA\r\n5     0     4    NA\r\n6     0     5    NA\r\n\r\n\r\nThis is what the plot now looks like:\r\n\r\n\r\n\r\nggplot(df, aes(x=PtsW, y=PtsL, fill=sqrt(Count))) +\r\n  geom_tile(color='black') +\r\n  geom_text(aes(label=Count), size=1.8, color='black')+\r\n  scale_fill_continuous(low=\"#FFF973\", high=\"#F92A0D\", na.value=\"white\") +\r\n  scale_y_reverse(breaks=seq(0,56,2)) +\r\n  scale_x_continuous(breaks=seq(0,76,2),position = \"top\")+\r\n  xlab(\"Winning Team Points\") +\r\n  ylab(\"Losing Team Points\") +\r\n  theme(\r\n    plot.background = element_rect(fill=\"white\"),\r\n    panel.background = element_rect(fill=\"white\"),\r\n    panel.border = element_rect(fill=NA, color=\"white\", size=0.15, linetype=\"solid\"),\r\n    axis.text = element_text(color=\"black\", size=rel(0.7)),\r\n    axis.text.y  = element_text(hjust=1),\r\n    legend.position = \"none\"\r\n     )\r\n\r\n\r\n\r\n\r\n\r\nThis is pretty interesting. There aren’t many scores along the 1,2,4,5 and 11 points rows/columns.\r\nOne final thing I wondered was what if we looked at total points for games. How many times has each combined point score been achieved.\r\n\r\n\r\ndf$Count <- ifelse(is.na(df$Count), 0, df$Count)\r\ndf$Total <- df$PtsW + df$PtsL\r\n\r\ndf.total <- \r\n  df %>% \r\n  group_by(Total) %>%\r\n  summarise(Count = sum(Count))\r\n\r\n\r\nhead(df.total)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  Total Count\r\n  <int> <dbl>\r\n1     0    73\r\n2     1     0\r\n3     2     5\r\n4     3    59\r\n5     4     0\r\n6     5     5\r\n\r\n\r\n\r\n\r\nggplot(df.total, aes(x=Total, y=Count)) +\r\n  geom_col(color='black',fill='salmon') +\r\n  ylab(\"Frequency\") +\r\n  xlab(\"Total Score\") +\r\n  ggtitle(\"Frequency of Pro Football Point Totals\") +\r\n  theme_classic()\r\n\r\n\r\n\r\n\r\n\r\nWe can also identify those scores that have never occurred.\r\n\r\n\r\ndf.total %>%\r\n  filter(Count==0)\r\n\r\n\r\n# A tibble: 25 x 2\r\n   Total Count\r\n   <int> <dbl>\r\n 1     1     0\r\n 2     4     0\r\n 3    92     0\r\n 4   100     0\r\n 5   102     0\r\n 6   104     0\r\n 7   107     0\r\n 8   108     0\r\n 9   109     0\r\n10   110     0\r\n# ... with 15 more rows\r\n\r\nWhat’s interesting here is that after total scores of 1 and 4, the next highest total score that has never occurred is 92!\r\nThere are other things we could look at with these data such as what scores have never occurred based on who is home/on the road. Also we could look at the rate of unique scores - how often have they occured over time.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-03-nfl-scorelines/nflplot.png",
    "last_modified": "2021-10-03T17:27:54-05:00",
    "input_file": {},
    "preview_width": 500,
    "preview_height": 417
  },
  {
    "path": "posts/2020-10-21-animated-covid-plots/",
    "title": "Animated Covid Plots",
    "description": "Scraping Texas Covid data and making animated plots",
    "author": [
      {
        "name": "James Curley",
        "url": "jamescurley.blog"
      }
    ],
    "date": "2020-10-21",
    "categories": [
      "time_series",
      "animation"
    ],
    "contents": "\r\nIn this post I shall show how to scrape Texas covid data and create animated plots. The purpose is to demonstrate how this can all be done seamlessly with R, without the need to download or manually edit the excel files in any way.\r\nTexas covid data is stored at this website. For this post we will focus on the link “Cases over Time by County”. This link leads to an excel sheet with covid cases by county. Unfortunately, like many excel files, it is filled with random text that isn’t data - e.g. titles, notes etc. We will need to edit the file before working with the data.\r\nImporting the Data\r\nTo directly import an excel file that lives at a web link we need the RCurl package. We download the file into a temporary location (our working directory), and then we read that with read_xlsx() from the readxl package. Because I’m using a Windows machine, I need to include ,  mode = \"wb\" at the end of the download.file() function. If you’re on a different operating system, you could delete this bit.\r\n\r\n\r\nlibrary(RCurl)\r\nlibrary(readxl)\r\n\r\n# for Windows, mode = 'wb'\r\ndownload.file(\"https://dshs.texas.gov/coronavirus/TexasCOVID19DailyCountyCaseCountData.xlsx\", \"temp.xlsx\", mode = \"wb\")\r\ntmp <- read_xlsx(\"temp.xlsx\")\r\n\r\n\r\nCleaning the Data\r\nWe have our excel file imported as a dataframe in R called tmp. Let’s look at its size, and then we’ll look at the first 5 rows and 4 columns:\r\n\r\n\r\ndim(tmp)\r\n\r\n[1] 267 230\r\n\r\ntmp[1:5, 1:4]\r\n\r\n# A tibble: 5 x 4\r\n  `COVID-19 Total Cases b~ ...2           ...3           ...4         \r\n  <chr>                    <chr>          <chr>          <chr>        \r\n1 DISCLAIMER: All data ar~  <NA>           <NA>           <NA>        \r\n2 County Name              \"Cases \\r\\r\\r~ \"Cases \\r\\r\\r~ \"Cases \\r\\r\\~\r\n3 Anderson                 \"0.0\"          \"0.0\"          \"0.0\"        \r\n4 Andrews                  \"0.0\"          \"0.0\"          \"0.0\"        \r\n5 Angelina                 \"0.0\"          \"0.0\"          \"0.0\"        \r\n\r\n\r\nAs you can see, the top of the dataframe is messy. The colnames are nonsensical. The first row is also not related to anything we need. The second row appears to have something of value. It appears that the third row is when our county names and data begin.\r\nLet’s look at the second row in more detail. We can look at the first three entries:\r\n\r\n\r\nas.character(tmp[2,1:3])\r\n\r\n[1] \"County Name\"                                                                                                                                                                                                    \r\n[2] \"Cases \\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n03-04\"\r\n[3] \"Cases \\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n03-05\"\r\n\r\n\r\nThe first entry is what should be the column name of column 1 - the county name. The other entries have a lot of random text (a remnant of some excel formatting) and some numbers at the end. Indeed, the last five characters refer to the date. If we grab the last five characters of each entry in this row, we will have our dates. When doing this, I also noticed that two dates (July 15th and July 17th) had asterisks at the end also. This is because down at the bottom of the excel book there were footnotes relating to these dates - some extra data were included on these dates.\r\nWe can grab all the dates by getting the last 5 characters (using str_sub() from stringr) and ignoring asterisks (using gsub() - and we have to do \\\\* to make sure that it really knows we mean asterisks. We’ll make these our new column names:\r\n\r\n\r\ncolnames(tmp) <- stringr::str_sub(gsub(\"\\\\*\", \"\", tmp[2,]),-5)\r\n\r\n\r\nWe can now get rid of the nonsense top two rows, and take a look at our data:\r\n\r\n\r\ntmp <- tmp[-c(1:2),]\r\n\r\ntmp[1:5, 1:4]\r\n\r\n# A tibble: 5 x 4\r\n  ` Name`  `03-04` `03-05` `03-06`\r\n  <chr>    <chr>   <chr>   <chr>  \r\n1 Anderson 0.0     0.0     0.0    \r\n2 Andrews  0.0     0.0     0.0    \r\n3 Angelina 0.0     0.0     0.0    \r\n4 Aransas  0.0     0.0     0.0    \r\n5 Archer   0.0     0.0     0.0    \r\n\r\n\r\nThese excel worksheets often have garbage at the bottom of the data - let’s check this:\r\n\r\n\r\ntmp[250:nrow(tmp), 1:3]\r\n\r\n# A tibble: 16 x 3\r\n   ` Name`                                             `03-04` `03-05`\r\n   <chr>                                               <chr>   <chr>  \r\n 1 \"Wood\"                                              0.0     0.0    \r\n 2 \"Yoakum\"                                            0.0     0.0    \r\n 3 \"Young\"                                             0.0     0.0    \r\n 4 \"Zapata\"                                            0.0     0.0    \r\n 5 \"Zavala\"                                            0.0     0.0    \r\n 6 \"Total\"                                             0.0     0.0    \r\n 7  <NA>                                               <NA>    <NA>   \r\n 8 \"Counties Reporting Cases\"                          0.0     0.0    \r\n 9  <NA>                                               <NA>    <NA>   \r\n10 \"Notes:\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\~ <NA>    <NA>   \r\n11 \"Case counts do not include probable cases\"         <NA>    <NA>   \r\n12 \"County-level case counts were not available on Ma~ <NA>    <NA>   \r\n13 \"Population data is based on Texas population proj~ <NA>    <NA>   \r\n14 \"* Texas is reporting 10,791 new confirmed COVID-1~ <NA>    <NA>   \r\n15 \"** Texas is reporting 10,256 new confirmed COVID-~ <NA>    <NA>   \r\n16  <NA>                                               <NA>    <NA>   \r\n\r\n\r\nIt looks like after the county Zavala, we have several rows of stuff we don’t need. These are totals, sub-totals and footnotes. As we want to make this code reproducible - and the authors of the excel sheet may add more footnotes in the future - we’ll tell R to locate ‘Zavala’ and cut-off the datasheet there. We’ll also call the dataframe df as it’s now clean:\r\n\r\n\r\ndf <- tmp[1:which(tmp[,1]==\"Zavala\"),]\r\ndf[1:5,1:4]\r\n\r\n# A tibble: 5 x 4\r\n  ` Name`  `03-04` `03-05` `03-06`\r\n  <chr>    <chr>   <chr>   <chr>  \r\n1 Anderson 0.0     0.0     0.0    \r\n2 Andrews  0.0     0.0     0.0    \r\n3 Angelina 0.0     0.0     0.0    \r\n4 Aransas  0.0     0.0     0.0    \r\n5 Archer   0.0     0.0     0.0    \r\n\r\n\r\nThe final thing we’d like to do to help with plotting and summary stats, is to conver this dataset from wide to long.\r\nWe’ll also make the first column name say ‘county’ and make sure the format of the date, and the value (number of Covid cases) are correct:\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\ndf.long <- df %>% pivot_longer(2:ncol(df), names_to = \"date\")\r\n\r\ncolnames(df.long)[1]<-\"county\"\r\n\r\ndf.long$date <- as.Date(df.long$date, format = \"%m-%d\")\r\n\r\ndf.long$value <- as.numeric(df.long$value)\r\n\r\nhead(df.long)\r\n\r\n# A tibble: 6 x 3\r\n  county   date       value\r\n  <chr>    <date>     <dbl>\r\n1 Anderson 2020-03-04     0\r\n2 Anderson 2020-03-05     0\r\n3 Anderson 2020-03-06     0\r\n4 Anderson 2020-03-09     0\r\n5 Anderson 2020-03-10     0\r\n6 Anderson 2020-03-11     0\r\n\r\ntail(df.long)\r\n\r\n# A tibble: 6 x 3\r\n  county date       value\r\n  <chr>  <date>     <dbl>\r\n1 Zavala 2020-10-16   419\r\n2 Zavala 2020-10-17   436\r\n3 Zavala 2020-10-18   436\r\n4 Zavala 2020-10-19   438\r\n5 Zavala 2020-10-20   443\r\n6 Zavala 2020-10-21   452\r\n\r\n\r\nVisualizing\r\nFor a first plot, we’ll use tidyverse to count up all the covid cases on each day for all counties of Texas. Then we’ll plot those data as a line graph. We’re using scales here to make sure the y-axis has readable numbers:\r\n\r\n\r\nlibrary(scales)\r\n\r\ndf.long %>%\r\n  group_by(date) %>%\r\n  summarise(total = sum(value)) %>%\r\n  ggplot(aes(x=date, y = total)) + \r\n  geom_line(color=\"#123abc\", lwd=1) +\r\n  theme_classic() +\r\n  scale_y_continuous(labels = comma_format()) +\r\n  ylab(\"Total Cases\") +\r\n  xlab(\"\") +\r\n  ggtitle(\"Number of Covid Cases in Texas 2020\") +\r\n  theme(axis.title = element_text(size=16))\r\n\r\n\r\n\r\nNow we are going to set ourselves the task of animating the Covid cases over time. We don’t want to do it for all 254 counties, so we will pick the five counties that have the most cases. We’ll use tidyverse to group by county to find the total cases per county:\r\n\r\n\r\n# let's pick the five highest counties.\r\n\r\ndf.long %>%\r\n  group_by(county) %>%\r\n  summarise(total = sum(value)) %>%\r\n  arrange(-total)\r\n\r\n# A tibble: 254 x 2\r\n   county       total\r\n   <chr>        <dbl>\r\n 1 Harris    12165234\r\n 2 Dallas     7825756\r\n 3 Tarrant    4406921\r\n 4 Bexar      4321739\r\n 5 Travis     2962018\r\n 6 Hidalgo    2644776\r\n 7 El Paso    2333416\r\n 8 Cameron    1924428\r\n 9 Fort Bend  1489174\r\n10 Nueces     1436065\r\n# ... with 244 more rows\r\n\r\n\r\nWe can now grab the five counties with the most cases, and then filter our long dataframe to only include those counties using %in%:\r\n\r\n\r\n# we can automatically grab these like this:\r\n\r\ndf.long %>%\r\n  group_by(county) %>%\r\n  summarise(total = sum(value)) %>%\r\n  arrange(-total) %>%\r\n  .$county %>%\r\n  head(5) -> my_counties\r\n\r\nmy_counties\r\n\r\n[1] \"Harris\"  \"Dallas\"  \"Tarrant\" \"Bexar\"   \"Travis\" \r\n\r\n#only keep data with these\r\n\r\ndf.long %>%\r\n  filter(county %in% my_counties) -> df.x\r\n\r\nhead(df.x)\r\n\r\n# A tibble: 6 x 3\r\n  county date       value\r\n  <chr>  <date>     <dbl>\r\n1 Bexar  2020-03-04     0\r\n2 Bexar  2020-03-05     0\r\n3 Bexar  2020-03-06     0\r\n4 Bexar  2020-03-09     0\r\n5 Bexar  2020-03-10     0\r\n6 Bexar  2020-03-11     0\r\n\r\ntail(df.x)\r\n\r\n# A tibble: 6 x 3\r\n  county date       value\r\n  <chr>  <date>     <dbl>\r\n1 Travis 2020-10-16 30688\r\n2 Travis 2020-10-17 30797\r\n3 Travis 2020-10-18 30908\r\n4 Travis 2020-10-19 30956\r\n5 Travis 2020-10-20 31053\r\n6 Travis 2020-10-21 31159\r\n\r\n\r\nNow to animate. We will use the gganimate package. This should work “out-of-the-box”, however, you may need to also install the av or gifski packages to ensure that it doesn’t just make hundreds of still images, but instead compiles them into an animation. The only bit of code we need to add to an otherwise static plot is the line + transition_reveal(date) which means we are animating along our x-axis (date). We use anim_save() to save the animation\r\n\r\n\r\n# May take a few seconds\r\nggplot(df.x, aes(x=date, y=value, color=county)) +\r\n  geom_line(lwd=1) +\r\n  geom_point() +\r\n  scale_color_manual(values=c(\"red\", \"black\", \"purple\", \"blue\",\"orange\"))+\r\n  ggtitle(\"Cumulative Covid Cases \\n for Selected Texas Counties\") +\r\n  theme_classic() +\r\n  ylab(\"Cumulative Covid Cases\") +\r\n  xlab(\"\")+\r\n  theme(\r\n    axis.title = element_text(size=16),\r\n    plot.title = element_text(size=20)\r\n    )+\r\n  transition_reveal(date) \r\n\r\n# Save as a gif:\r\nanim_save(\"covid.gif\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-21-animated-covid-plots/covid.gif",
    "last_modified": "2021-10-03T13:41:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-20-tsa-passenger-screenings-during-covid/",
    "title": "TSA passenger screenings during Covid",
    "description": "Examining how total USA airline passenger changes following the Covid pandemic",
    "author": [
      {
        "name": "James Curley",
        "url": "jamescurley.blog"
      }
    ],
    "date": "2020-10-20",
    "categories": [
      "time_series"
    ],
    "contents": "\r\nClearly air traffic plummeted during Covid. Bob Rudis tweeted a great figure showing how passenger numbers changed between 2019 and 2020. I wanted to recreate the figure here. It’s a good exercise in web scraping data, data cleaning, and how to plan and actually make appealing and informative time-series line graphs with customized ggplot2 elements.\r\nThe data\r\nThe data are available at the TSA website here. They appear to be updated daily, so hopefully the code below should work no matter what day the data are downloaded.\r\nWe will use the rvest package to read in the url and scrape the HTML table. The extract2() function comes from magrittr and helps us get the first (and only) element from the list produced by html_table().\r\n\r\n\r\nlibrary(rvest)\r\nlibrary(tidyverse)\r\nlibrary(magrittr)\r\n\r\nurl <- \"https://www.tsa.gov/coronavirus/passenger-throughput\"\r\n\r\ndf <- read_html(url) %>%\r\n  html_table() %>%\r\n  extract2(1)\r\n\r\nhead(df)\r\n\r\n        Date Total Traveler Throughput\r\n1 10/19/2020                   921,031\r\n2 10/18/2020                 1,031,505\r\n3 10/17/2020                   788,743\r\n4 10/16/2020                   973,046\r\n5 10/15/2020                   950,024\r\n6 10/14/2020                   717,940\r\n  Total Traveler Throughput (1 Year Ago - Same Weekday)\r\n1                                             2,514,673\r\n2                                             2,606,266\r\n3                                             2,049,855\r\n4                                             2,637,667\r\n5                                             2,581,007\r\n6                                             2,317,763\r\n\r\n\r\nWe have three columns: date, total passengers in 2020, and total passengers on the corresponding weekday of 2019. So, the final column is not the exact same calendar data - but the corresponding closest Monday, Tuesday, etc. of 2019.\r\nFirst, let’s clean up the column names:\r\n\r\n\r\ncolnames(df) <- c(\"date\", \"pass2020\", \"pass2019\")\r\nhead(df)\r\n\r\n        date  pass2020  pass2019\r\n1 10/19/2020   921,031 2,514,673\r\n2 10/18/2020 1,031,505 2,606,266\r\n3 10/17/2020   788,743 2,049,855\r\n4 10/16/2020   973,046 2,637,667\r\n5 10/15/2020   950,024 2,581,007\r\n6 10/14/2020   717,940 2,317,763\r\n\r\n\r\nWe still have some cleaning steps. We need to fix the date, the numbers and to reshape the dataframe.\r\nLet’s make the date a date object:\r\n\r\n\r\ndf$date <- as.Date(df$date, format=\"%m/%d/%Y\")\r\nstr(df) #it's now a 'date' column\r\n\r\n'data.frame':   233 obs. of  3 variables:\r\n $ date    : Date, format: \"2020-10-19\" ...\r\n $ pass2020: chr  \"921,031\" \"1,031,505\" \"788,743\" \"973,046\" ...\r\n $ pass2019: chr  \"2,514,673\" \"2,606,266\" \"2,049,855\" \"2,637,667\" ...\r\n\r\n Next, we’ll remove commas from the numbers and ensure the columns are numeric - you can see from the output above that R still thinks they are characters:\r\n\r\n\r\ndf$pass2020 <- as.numeric(gsub(\",\", \"\", df$pass2020))\r\ndf$pass2019 <- as.numeric(gsub(\",\", \"\", df$pass2019))\r\nhead(df)\r\n\r\n        date pass2020 pass2019\r\n1 2020-10-19   921031  2514673\r\n2 2020-10-18  1031505  2606266\r\n3 2020-10-17   788743  2049855\r\n4 2020-10-16   973046  2637667\r\n5 2020-10-15   950024  2581007\r\n6 2020-10-14   717940  2317763\r\n\r\n\r\nTo be able to plot these data with ggplot2, we need to convert this dataframe from a wide dataframe to a long dataframe:\r\n\r\n\r\ndf <- df %>% \r\n  pivot_longer(cols=2:3)\r\n\r\ndf\r\n\r\n# A tibble: 466 x 3\r\n   date       name       value\r\n   <date>     <chr>      <dbl>\r\n 1 2020-10-19 pass2020  921031\r\n 2 2020-10-19 pass2019 2514673\r\n 3 2020-10-18 pass2020 1031505\r\n 4 2020-10-18 pass2019 2606266\r\n 5 2020-10-17 pass2020  788743\r\n 6 2020-10-17 pass2019 2049855\r\n 7 2020-10-16 pass2020  973046\r\n 8 2020-10-16 pass2019 2637667\r\n 9 2020-10-15 pass2020  950024\r\n10 2020-10-15 pass2019 2581007\r\n# ... with 456 more rows\r\n\r\n\r\nNow we can graph. We’ll first make the basic graph, and then we can add the customization:\r\n\r\n\r\nggplot(df, aes(x=date, y=value, color=name)) + \r\n  geom_line()\r\n\r\n\r\n\r\nThere are lots of interesting patterns in that graph. Clearly, number one is that the passenger numbers in mid March 2020 showed a dramatic fall, and a very slow recovery. The rate of recovery seems to have slowed down between July 2020 and October 2020. Starting in October 2020 we’re perhaps seeing a slightly increase again in the rate of passenger increase. The other patterns of interest are the weekly cycle of passenger numbers - an obvious pattern, but always notable to observe, as well as the season variation in traffic that usually occur in normal years.\r\nTo make this chart more readable, we want to change the passenger numbers on the y-axis from ‘e’ numbers to understandable numbers, add a title, remove the legend, change the theme and the line colors. I will also make the lines thicker and change the font size of the text on the figure. We should also source the data and original author.\r\n\r\n\r\nlibrary(scales) # needed to reformat the y-axis with commas in the numbers\r\n\r\nggplot(df, aes(x=date, y=value, color=name)) + \r\n  geom_line(lwd=1) +\r\n  scale_color_manual(values=c(\"#f59a46\", \"#123abc\"))+\r\n  scale_y_continuous(breaks = seq(0,3000000, 500000), labels = comma) +\r\n  ylab(\"\") +\r\n  xlab(\"\") +\r\n  ggtitle(\"TSA checkpoint travel numbers for 2020 and 2019\",\r\n          subtitle = \"Total Traveler Throughput; Same Weekday\") +\r\n  theme_minimal() +\r\n  labs(caption = \"Adapated from Figure by Bob Rudis; Data: https://www.tsa.gov/coronavirus/passenger-throughput\") +\r\n  theme(\r\n    legend.position = 'none',\r\n    axis.text = element_text(size=12),\r\n    plot.title = element_text(size=18)\r\n    )\r\n\r\n\r\n\r\nThis looks a lot better but it still has one major issue, which is you don’t know which line is 2019 and which line is 2020. Well, you could take a guess, but it should be keyed in the figure. I want to change colors in the title of the plot to indicate this.\r\nThis is a little trickier and requires a package called ggtext which is downloadable from here. The code you need to include requires you to use a little bit of HTML tagging, but the overall effect is worth it. More examples of using ggtext can be found on it’s GitHub page. Here is a useful example also on stackoverflow.\r\n\r\n\r\nlibrary(ggtext) #remotes::install_github(\"wilkelab/ggtext\")\r\n\r\nggplot(df, aes(x=date, y=value, color=name)) +\r\n  geom_line(lwd = 1) +\r\n  scale_color_manual(\r\n    name = NULL,\r\n    values = c(pass2019 = \"#f59a46\", pass2020 = \"#123abc\"),\r\n    labels = c(\r\n      pass2019 = \"<i style='color:#f59a46'>I. pass2019<\/i>\",\r\n      pass2020 = \"<i style='color:#123abc'>I. pass2020<\/i>\")\r\n  ) +\r\n  labs(\r\n    title = \"<span style='font-size:18pt'>**TSA checkpoint travel numbers for <span style='color:#123abc;'>2020<\/span>\r\n    and <span style='color:#f59a46;'>2019<\/span>**<\/span>  \r\n    <span style='font-size:12pt'>Total Traveler Throughput; Same Weekday<\/span>\r\n    \",\r\n    x = \"Sepal length (cm)\", y = \"Sepal width (cm)\"\r\n  ) +\r\n  scale_y_continuous(breaks = seq(0,3000000, 500000), labels = comma) +\r\n  ylab(\"\") +\r\n  xlab(\"\")+\r\n  theme_minimal() +\r\n  labs(caption = \"Adapated from Figure by Bob Rudis; Data: https://www.tsa.gov/coronavirus/passenger-throughput\")+\r\n  theme(\r\n    plot.title = element_markdown(lineheight = 1.1),\r\n     legend.position = 'none',\r\n    axis.text = element_text(size=12)\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-20-tsa-passenger-screenings-during-covid/tsa-passenger-screenings-during-covid_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2021-10-03T13:41:28-05:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 960
  },
  {
    "path": "posts/2020-10-16-apportionment-methods/",
    "title": "Apportionment Methods",
    "description": "Discussion of apportionment problems and the rabbit hole I went down.",
    "author": [
      {
        "name": "James Curley",
        "url": "jamescurley.blog"
      }
    ],
    "date": "2020-10-17",
    "categories": [
      "puzzles",
      "politics",
      "history",
      "Rpackage"
    ],
    "contents": "\r\nThis blog post was originally written in 2015 - I’m just migrating it here now. I had forgotten all about this - it’s been sort-of fun to go through this again.\r\nWarning: This is a long post. The big picture can be gotten from the first few paragraphs. After that, I dive into all the various functions I wrote to solve apportionment problems as applied to US government.\r\nI think the moral of this story, is that sometimes you find a scratch you need to itch, and you can’t let it go and it ends up with you making an R package for something that you don’t really have a need for.\r\nSo why did I start to write an apportionment package that works out how to best apportion seats by state to the US House of Representatives? It seems illogical as I am not an American, I am not a political scientist, and if I’m honest, I don’t fully understand the American political system or really have much interest in it. Further, I’d never even heard of apportionment as a thing until I fell down this rabbit hole.\r\nThis is a classic case of having one simple question that I wanted to answer and then being led down a path of ever expanding further questions. It started with me playing around with Bob Rudis’ excellent waffle package. I’ll walk you through the steps…\r\n\r\nHere’s what happened:\r\nI was interested in plotting data such as the following example. Here there are seven groups A to G, with each one possessing an integer value that are stored in a vector x. The sum of x is 255. Using the waffle() function, I am plotting this data with each square representing ‘5’ of the total sum. I’m also using 5 rows to plot the data.\r\n\r\n\r\nlibrary(waffle)\r\n\r\nset.seed(10)\r\nx <- round(runif(7, 1, 100),0)\r\nnames(x) <- LETTERS[1:7]\r\nx\r\n\r\n A  B  C  D  E  F  G \r\n51 31 43 70  9 23 28 \r\n\r\nsum(x)\r\n\r\n[1] 255\r\n\r\nwaffle(x/5, rows=5)\r\n\r\n\r\nIt looks pretty. What you might notice is that we have 48 squares meaning that 240 of our total sum are represented (i.e. 48∗5). Or, 15, which would equal 2 squares are not. Breaking it down by group, you can see that there are 10 squares for A meaning that ‘1’ of A’s total of 51 is not represented, 6 squares of B leaving out ‘1’ of B’s total of 31, 8 squares of C meaning that 3 of C’s total of 43 are lost, etc. etc. As D is the only group whose value is divisible by 5, it is the only one whose squares exactly represent its value. The way waffle works is to take the floor or rounded-down value of each groups number (as far as I can tell).\r\nMy other interest was in creating charts that represent percentage plots. i.e. to convert each vector into percentages and then to plot those. Like this:\r\n\r\n\r\nx1 <- (100*x)/sum(x)\r\nx1\r\n\r\n        A         B         C         D         E         F         G \r\n20.000000 12.156863 16.862745 27.450980  3.529412  9.019608 10.980392 \r\n\r\nwaffle(x1, rows=5)\r\n\r\n\r\nThe first thing to note is that there obviously aren’t 100 squares - there are 97. If we go group by group, A is perfectly represented by 20 blocks as it’s percentage is exactly 20%. For each of the others, you’ll notice that the number of squares for each group is the integer part of each percentage (or the floor / rounded down value in other terms).\r\nSumming the fractional parts shows that indeed the missing 3 squares are the fractional parts of each percentage:\r\n\r\n\r\n#return fractional parts\r\nx1%%1 \r\n\r\n         A          B          C          D          E          F \r\n0.00000000 0.15686275 0.86274510 0.45098039 0.52941176 0.01960784 \r\n         G \r\n0.98039216 \r\n\r\nsum(x1%%1)\r\n\r\n[1] 3\r\n\r\n\r\nObviously, this method of visualizing these data is perfectly appropriate. It may well be (and often will be) just wrong to have extra squares representing a whole percentage point. However, we could easily concede in our example that group G and group C with fractions of .98 and .86 should get an extra square, whereas group F and group B with fractions of .01 and .16 should not. It would be difficult to decide whether E with a fractional of .53 or D with a fractional of .45 should get the last extra square.\r\nBecause I wanted to produce a number of waffle plots to compare, I did actually want my graphs to be perfect rectangles of 5 rows by 20 columns and for these extra squares to be allocated. The next question was obviously - what’s the best or fairest method of doing this?\r\nUnsurprisingly to many of you, I had stumbled upon the apportionment paradox that has a long history in American and other countries’ political systems.\r\nThe apportR package was therefore my way of collating all the various methods that have been proposed to solve this problem. And it turned out there are an annoyingly large number of methods. I’m not going to detail the code behind each of the functions - but I do explain the logic behind each one. If you’re interested in how the functions are computing the logic, then please go to the apportR homepage - and you can see the functions there.\r\n\r\n“Fair Division” examples\r\nI will illustrate the general problem with three examples. Each of these datasets comes from my apportR package. The apportR package can be downloaded from github.\r\n\r\n\r\n\r\nlibrary(devtools)\r\ndevtools::install_github(\"jalapic/apportR\")\r\n\r\n\r\n\r\n\r\nlibrary(apportR)\r\n\r\n\r\nHousework\r\nHere is a common example used to illustrate the problem at hand.\r\n\r\nMom has 50 identical pieces of candy that cannot be broken into bits. She tells her five children that she will divide the candy fairly at the end of the week according to the proportion of housework that each performs.\r\n\r\nHere are the minutes worked by each child:\r\n\r\n\r\nhousework\r\n\r\n Anna Betty  Carl Derek  Ella \r\n  150    78   173   204   295 \r\n\r\n#calculate how many candies each should get\r\ncandies <- (50*housework) / sum(housework)\r\ncandies\r\n\r\n     Anna     Betty      Carl     Derek      Ella \r\n 8.333333  4.333333  9.611111 11.333333 16.388889 \r\n\r\nfloor(candies)\r\n\r\n Anna Betty  Carl Derek  Ella \r\n    8     4     9    11    16 \r\n\r\nsum(floor(candies))\r\n\r\n[1] 48\r\n\r\nwaffle(candies, rows=5) \r\n\r\n\r\n\r\nAnna expects 8.33%, Betty 4.33%, Carl 9.6%, Derek 11.33% and Ella 16.39% of the candies, but they cannot be broken up. Assuming each gets the number of candies equal to their rounded down proportion at least, then Who should get the extra 2 candies that are not yet allocated? Obviously, it’s very hard to work out who. That’s the problem at hand ! (It also turns out this problem is quite contentious when we are dealing with electoral seats and not candies).\r\n\r\nScholarships\r\n\r\nThere are 12 scholarships to be given out to each of 3 subjects at a university. They are to be awarded based upon the proportion of majors that each subject has.\r\n\r\n\r\nHere are the majors in each subject as well as the expected number of scholarships each would get out of the 12 available:\r\n\r\n\r\n\r\n   English    History Psychology \r\n       231        502        355 \r\n\r\n   English    History Psychology \r\n  2.547794   5.536765   3.915441 \r\n\r\n   English    History Psychology \r\n         2          5          3 \r\n\r\n[1] 10\r\n\r\n\r\nIf each gets the rounded down value that they would expect, where do the remaining 2 scholarships go ? Presumably Psychology should get an extra one as its fractional is .9, but which of English and History should get the other one?\r\n\r\nHouse of Representatives\r\n\r\nAt the 1792 US House of Representatives elections, a total of 105 seats were to be awarded that would be shared between the 15 states of the union proportional to each state’s population size at the 1790 US census.\r\n\r\nMore reading can be found here. Here are the states and their population sizes in 1790, followed by the expected number of seats for each state based on there being 105 available:\r\n\r\n\r\n\r\n    VA     MA     PA     NC     NY     MD     CT     SC     NJ     NH \r\n630560 475327 432879 353523 331589 278514 236841 206236 179570 141822 \r\n    VT     GA     KY     RI     DE \r\n 85533  70835  68705  68446  55540 \r\n\r\n       VA        MA        PA        NC        NY        MD        CT \r\n18.310361 13.802666 12.570050 10.265690  9.628765  8.087560  6.877449 \r\n       SC        NJ        NH        VT        GA        KY        RI \r\n 5.988733  5.214399  4.118263  2.483729  2.056925  1.995073  1.987552 \r\n       DE \r\n 1.612785 \r\n\r\nVA MA PA NC NY MD CT SC NJ NH VT GA KY RI DE \r\n18 13 12 10  9  8  6  5  5  4  2  2  1  1  1 \r\n\r\n[1] 97\r\n\r\n\r\nWho should get those 8 extra seats ??? This is a slightly more important question to get right than the fair division of candies (depending upon your perspective)!\r\nI will describe some methods for working this out as well as the functions contained in apportR to represent these methods a bit later in this vignette.\r\n\r\nApportionment Methods Overview\r\nThe methods for solving these sorts of questions essentially fall into two broad groups. There are the standard (or non-divisor) methods and the divisor methods. Many (if not all) of them have been discovered independently in various disciplines and so each may have several different names. I will mainly focus on the methods from the historical perspective of the US House of Representatives apportionment problem, as it is the most interesting.\r\n\r\nStandard Non-Divisor Methods\r\nThe two main standard non-divisor methods are:\r\nHamilton’s Method, aka Largest Remainder Method, Vinton’s Method, Hare-Niemeyer Method.\r\nLowndes’ Method\r\n\r\nHamilton Method\r\nThe most simple method was first suggested by Alexander Hamilton in 1792.\r\nQuite straightforwardly, the logic of this method is as follows:\r\nchoose the number of seats available\r\ncalculate the standard divisor (this is equal to the total population / number of seats)\r\ncalculate the standard quota (this is equal to each state population / standard divisor)\r\nassign each state initially its lower quota (this is the rounded down value of each standard quota)\r\ncalculate how many seats are still to be assigned\r\nassign extra seats to those states with the highest fractionals from their standard quota in descending order.\r\n\r\nExample\r\nThere were 120 seats to be allocated in the House in 1790. The population of each state and the total US population at the 1790 census was as follows:\r\n\r\n\r\n\r\nusa1790\r\n\r\n    VA     MA     PA     NC     NY     MD     CT     SC     NJ     NH \r\n630560 475327 432879 353523 331589 278514 236841 206236 179570 141822 \r\n    VT     GA     KY     RI     DE \r\n 85533  70835  68705  68446  55540 \r\n\r\nsum(usa1790)\r\n\r\n[1] 3615920\r\n\r\n\r\nThe standard divisor (which is equivalent to the average number of people that each representative is representing) and standard quotas of each state are therefore:\r\n\r\n\r\n\r\n#standard divisor\r\nstd <- sum(usa1790) / 120\r\nstd\r\n\r\n[1] 30132.67\r\n\r\nstq <- usa1790 / std\r\nstq\r\n\r\n       VA        MA        PA        NC        NY        MD        CT \r\n20.926127 15.774475 14.365771 11.732218 11.004303  9.242926  7.859942 \r\n       SC        NJ        NH        VT        GA        KY        RI \r\n 6.844266  5.959313  4.706586  2.838547  2.350771  2.280084  2.271488 \r\n       DE \r\n 1.843182 \r\n\r\n\r\nThe first allocation of seats is the rounded down value of these standard quotas. When summing these values, it is clear that they only sum to 111 seats which is 9 short of the required total of 120.\r\n\r\n\r\n\r\nlowq <- floor(stq)\r\nlowq\r\n\r\nVA MA PA NC NY MD CT SC NJ NH VT GA KY RI DE \r\n20 15 14 11 11  9  7  6  5  4  2  2  2  2  1 \r\n\r\nsum(lowq)\r\n\r\n[1] 111\r\n\r\n\r\nThe states are then arranged in order of their fractional parts and the states with the 9 largest fractionals are awarded an extra seat. That would be NJ to NC.\r\n\r\n\r\n\r\nrev(sort(stq%%1))\r\n\r\n         NJ          VA          CT          SC          DE \r\n0.959313259 0.926126684 0.859941592 0.844266466 0.843182371 \r\n         VT          MA          NC          NH          PA \r\n0.838547313 0.774475099 0.732217527 0.706586429 0.365771367 \r\n         GA          KY          RI          MD          NY \r\n0.350771035 0.280083630 0.271488307 0.242925728 0.004303193 \r\n\r\n\r\nThis can be done using the hamilton() function. The first argument is the named vector of population sizes and the second argument is the number of seats to allocate.\r\n\r\n\r\n\r\nhamilton(usa1790, 120)\r\n\r\nVA MA PA NC NY MD CT SC NJ NH VT GA KY RI DE \r\n21 16 14 12 11  9  8  7  6  5  3  2  2  2  2 \r\n\r\n\r\nThe 1792 bill proposing this method be used was passed but yet it was not brought into use at this time. Instead, this bill was the very first usage of a Presidential veto by George Washington and a divisor method proposed by Thomas Jefferson was used. I will discuss this method in the divisor methods section. Washington vetoed the bill as Hamilton’s method would have meant that some representatives would be representing fewer than 30,000 individuals which was against the constitution. It was first of only two times that Washington used his Presidential veto.\r\nWe can see this here - several states have too many representatives according to the Constitution:\r\n\r\n\r\nusa1790 / hamilton(usa1790, 120)\r\n\r\n      VA       MA       PA       NC       NY       MD       CT \r\n30026.67 29707.94 30919.93 29460.25 30144.45 30946.00 29605.12 \r\n      SC       NJ       NH       VT       GA       KY       RI \r\n29462.29 29928.33 28364.40 28511.00 35417.50 34352.50 34223.00 \r\n      DE \r\n27770.00 \r\n\r\n \r\nDespite this initial lack of support, Hamilton’s method (also known as Vinton’s method by the mid nineteenth century) was passed into law and adopted by 1850 and continued to be used until the turn of the 20th century. It stopped being used at this time because some strange anomalies or paradoxes were discovered. These are as follows:\r\n \r\nAlabama Paradox\r\nIn 1880 a government clerk named C. W. Seaton computed apportionments for all potential House sizes between 275 and 350 members. He then wrote to Congress describing that if the House of Representatives had 299 seats, Alabama would get 8 members but it would only receive 7 members if the House had 300 seats. This seemed to be a failure of common sense and became known as the Alabama Paradox.\r\nHere are the state populations for 1880 and the number of seats that would be given under the Hamilton method if the House had 299 or 300 members:\r\n\r\n\r\nusa1880\r\n\r\n       Alabama       Arkansas     California       Colorado \r\n       1262505         802525         864694         194327 \r\n   Connecticut       Delaware        Florida        Georgia \r\n        622700         146608         269493        1542180 \r\n      Illinois        Indiana           Iowa         Kansas \r\n       3077871        1978301        1624615         996096 \r\n      Kentucky      Louisiana          Maine       Maryland \r\n       1648690         939946         648936         934943 \r\n Massachusetts       Michigan      Minnesota    Mississippi \r\n       1783085        1636937         780773        1131597 \r\n      Missouri       Nebraska         Nevada  New Hampshire \r\n       2168380         452402          62266         346991 \r\n    New Jersey       New York North Carolina           Ohio \r\n       1131116        5082871        1399750        3198062 \r\n        Oregon   Pennsylvania   Rhode Island South Carolina \r\n        174768        4282891         276531         995577 \r\n     Tennessee          Texas        Vermont       Virginia \r\n       1542359        1591749         332286        1512565 \r\n West Virginia      Wisconsin \r\n        618457        1315497 \r\n\r\nhamilton(usa1880, 299)\r\n\r\n       Alabama       Arkansas     California       Colorado \r\n             8              5              5              1 \r\n   Connecticut       Delaware        Florida        Georgia \r\n             4              1              1              9 \r\n      Illinois        Indiana           Iowa         Kansas \r\n            18             12             10              6 \r\n      Kentucky      Louisiana          Maine       Maryland \r\n            10              6              4              6 \r\n Massachusetts       Michigan      Minnesota    Mississippi \r\n            11             10              5              7 \r\n      Missouri       Nebraska         Nevada  New Hampshire \r\n            13              3              1              2 \r\n    New Jersey       New York North Carolina           Ohio \r\n             7             31              8             19 \r\n        Oregon   Pennsylvania   Rhode Island South Carolina \r\n             1             26              2              6 \r\n     Tennessee          Texas        Vermont       Virginia \r\n             9              9              2              9 \r\n West Virginia      Wisconsin \r\n             4              8 \r\n\r\nhamilton(usa1880, 300)\r\n\r\n       Alabama       Arkansas     California       Colorado \r\n             7              5              5              1 \r\n   Connecticut       Delaware        Florida        Georgia \r\n             4              1              1              9 \r\n      Illinois        Indiana           Iowa         Kansas \r\n            19             12             10              6 \r\n      Kentucky      Louisiana          Maine       Maryland \r\n            10              6              4              6 \r\n Massachusetts       Michigan      Minnesota    Mississippi \r\n            11             10              5              7 \r\n      Missouri       Nebraska         Nevada  New Hampshire \r\n            13              3              1              2 \r\n    New Jersey       New York North Carolina           Ohio \r\n             7             31              8             19 \r\n        Oregon   Pennsylvania   Rhode Island South Carolina \r\n             1             26              2              6 \r\n     Tennessee          Texas        Vermont       Virginia \r\n             9             10              2              9 \r\n West Virginia      Wisconsin \r\n             4              8 \r\n\r\n \r\nTexas and Illinois would each gain a member whilst Alabama lost one !\r\n \r\nNew States Paradox\r\nA second paradox of this method also came to light. This is probably best illustrated using the example of Oklahoma which was admitted to the union in 1907. In 1900 the house had 386 seats. According to OK’s population, they should have got 5 seats.\r\nThe US population in 1900 and seat allocation according to Hamilton’s method was as follows:\r\n\r\n\r\nusa1900\r\n\r\n     NY      PA      IL      OH      MO      TX      MA      IN \r\n7264183 6302115 4821550 4157545 3106665 3048710 2805346 2516462 \r\n     MI      IA      GA      KY      WI      TN      NC      NJ \r\n2420982 2231853 2216331 2147174 2067385 2020616 1893810 1883669 \r\n     VA      AL      MN      MS      CA      KS      LA      SC \r\n1854184 1828697 1749626 1551270 1483504 1470495 1381625 1340316 \r\n     AR      MD      NE      WV      CT      ME      CO      FL \r\n1311564 1188044 1066300  958800  908420  694466  539103  528542 \r\n     WA      RI      OR      NH      SD      VT      ND      UT \r\n 515572  428556  413536  411588  390638  343641  314454  275277 \r\n     MT      DE      ID      WY      NV \r\n 232583  184735  159475   92531   40670 \r\n\r\nhamilton(usa1900, 386)\r\n\r\nNY PA IL OH MO TX MA IN MI IA GA KY WI TN NC NJ VA AL MN MS CA KS LA \r\n38 33 25 21 16 16 14 13 12 11 11 11 11 10 10 10 10  9  9  8  8  8  7 \r\nSC AR MD NE WV CT ME CO FL WA RI OR NH SD VT ND UT MT DE ID WY NV \r\n 7  7  6  5  5  5  3  3  3  3  2  2  2  2  2  2  1  1  1  1  1  1 \r\n\r\n \r\nbut the following happens when adding in Oklahoma and allowing for an extra 5 seats:\r\n\r\n\r\nusa1907\r\n\r\n     NY      PA      IL      OH      MO      TX      MA      IN \r\n7264183 6302115 4821550 4157545 3106665 3048710 2805346 2516462 \r\n     MI      IA      GA      KY      WI      TN      NC      NJ \r\n2420982 2231853 2216331 2147174 2067385 2020616 1893810 1883669 \r\n     VA      AL      MN      MS      CA      KS      LA      SC \r\n1854184 1828697 1749626 1551270 1483504 1470495 1381625 1340316 \r\n     AR      MD      NE      OK      WV      CT      ME      CO \r\n1311564 1188044 1066300 1000000  958800  908420  694466  539103 \r\n     FL      WA      RI      OR      NH      SD      VT      ND \r\n 528542  515572  428556  413536  411588  390638  343641  314454 \r\n     UT      MT      DE      ID      WY      NV \r\n 275277  232583  184735  159475   92531   40670 \r\n\r\nhamilton(usa1907, 391)\r\n\r\nNY PA IL OH MO TX MA IN MI IA GA KY WI TN NC NJ VA AL MN MS CA KS LA \r\n37 33 25 21 16 16 14 13 12 11 11 11 11 10 10 10 10  9  9  8  8  8  7 \r\nSC AR MD NE OK WV CT ME CO FL WA RI OR NH SD VT ND UT MT DE ID WY NV \r\n 7  7  6  5  5  5  5  4  3  3  3  2  2  2  2  2  2  1  1  1  1  1  1 \r\n\r\n \r\nAs you can see, after OK is added in 1907, New York would lose a seat and gives it to Maine! Again, this seems to fail a common sense test.\r\n \r\nPopulation Paradox\r\nA final paradox that has been found is that states that increase in population more relative to other states over a period of time can even lose seats.\r\nHere is a fictitious example. Say there are five states (A-E) and this is their population in hundreds of thousands at time 1 (states1) and time 2 (states2):\r\n\r\n\r\nstates1 <- c(150, 78, 173, 204, 295)\r\nstates2 <- c(150, 78, 181, 204, 296) #C increases by 8, E by 1\r\nnames(states1)<-names(states2)<-LETTERS[1:5]\r\n\r\nstates1\r\n\r\n  A   B   C   D   E \r\n150  78 173 204 295 \r\n\r\nstates2\r\n\r\n  A   B   C   D   E \r\n150  78 181 204 296 \r\n\r\n \r\nA, B and D have remained the same whilst E and C have increased in population. Now say there are 50 seats to allocate - let’s see what happens:\r\n\r\n\r\nhamilton(states1, 50)\r\n\r\n A  B  C  D  E \r\n 8  4 10 11 17 \r\n\r\nhamilton(states2, 50)\r\n\r\n A  B  C  D  E \r\n 8  5 10 11 16 \r\n\r\n \r\nEven though E’s population went up by 100,000 it actually lost a representative seat! whilst B whose population stayed the same increased its seat number and C whose population increased by 800,000 remained the same.\r\n \r\nStrengths & Weaknesses\r\nThe biggest advantage of this method is that it is very easy to calculate and to explain. This method also never violates the quota rule which states that the number of seats ought to be between the lower and upper quota of the standard quota (effectively the floor and ceiling value). The biggest weaknesses are that it throws up these strange paradoxes or common sense failures, and also that it does systematically favor larger states over smaller ones.\r\n \r\nAnother aside: The zero seats problem - for calculating seats in the House, there must always be at least one seat allocated to every state. The hamilton function does this automatically, so if for instance the standard quota of any state would be e.g. 0.333 and its fraction was not large enough to receive an extra surplus seat, then it will override this and allocate the seat. This happens several times in US census history for Delaware and Nevada.\r\nOther users of this function may want to have the option of allocating 0 to a group. Therefore the sister function to hamilton() is hamilton0() which allows for this.\r\nFor example, take the US census in 1880 and 300 available seats in the House:\r\n\r\n\r\nusa1880\r\n\r\n       Alabama       Arkansas     California       Colorado \r\n       1262505         802525         864694         194327 \r\n   Connecticut       Delaware        Florida        Georgia \r\n        622700         146608         269493        1542180 \r\n      Illinois        Indiana           Iowa         Kansas \r\n       3077871        1978301        1624615         996096 \r\n      Kentucky      Louisiana          Maine       Maryland \r\n       1648690         939946         648936         934943 \r\n Massachusetts       Michigan      Minnesota    Mississippi \r\n       1783085        1636937         780773        1131597 \r\n      Missouri       Nebraska         Nevada  New Hampshire \r\n       2168380         452402          62266         346991 \r\n    New Jersey       New York North Carolina           Ohio \r\n       1131116        5082871        1399750        3198062 \r\n        Oregon   Pennsylvania   Rhode Island South Carolina \r\n        174768        4282891         276531         995577 \r\n     Tennessee          Texas        Vermont       Virginia \r\n       1542359        1591749         332286        1512565 \r\n West Virginia      Wisconsin \r\n        618457        1315497 \r\n\r\nhamilton(usa1880, 300)\r\n\r\n       Alabama       Arkansas     California       Colorado \r\n             7              5              5              1 \r\n   Connecticut       Delaware        Florida        Georgia \r\n             4              1              1              9 \r\n      Illinois        Indiana           Iowa         Kansas \r\n            19             12             10              6 \r\n      Kentucky      Louisiana          Maine       Maryland \r\n            10              6              4              6 \r\n Massachusetts       Michigan      Minnesota    Mississippi \r\n            11             10              5              7 \r\n      Missouri       Nebraska         Nevada  New Hampshire \r\n            13              3              1              2 \r\n    New Jersey       New York North Carolina           Ohio \r\n             7             31              8             19 \r\n        Oregon   Pennsylvania   Rhode Island South Carolina \r\n             1             26              2              6 \r\n     Tennessee          Texas        Vermont       Virginia \r\n             9             10              2              9 \r\n West Virginia      Wisconsin \r\n             4              8 \r\n\r\nhamilton0(usa1880, 300)\r\n\r\n       Alabama       Arkansas     California       Colorado \r\n             8              5              5              1 \r\n   Connecticut       Delaware        Florida        Georgia \r\n             4              1              1              9 \r\n      Illinois        Indiana           Iowa         Kansas \r\n            19             12             10              6 \r\n      Kentucky      Louisiana          Maine       Maryland \r\n            10              6              4              6 \r\n Massachusetts       Michigan      Minnesota    Mississippi \r\n            11             10              5              7 \r\n      Missouri       Nebraska         Nevada  New Hampshire \r\n            13              3              0              2 \r\n    New Jersey       New York North Carolina           Ohio \r\n             7             31              8             19 \r\n        Oregon   Pennsylvania   Rhode Island South Carolina \r\n             1             26              2              6 \r\n     Tennessee          Texas        Vermont       Virginia \r\n             9             10              2              9 \r\n West Virginia      Wisconsin \r\n             4              8 \r\n\r\n \r\nClearly, Nevada would not get a seat unless allowances were made to ensure all states received at least one seat. Alabama would have lost out again in this instance !\r\n \r\nThis zero issue can also be illustrated with a couple of fair division problems. Say we had five candies to divide to each of 3 children dependent upon how many hours of work they completed.\r\n\r\n\r\nhours <- c(1,3,11)\r\nnames(hours) <- c(\"Xavier\", \"Yasmin\", \"Zach\")\r\nhours\r\n\r\nXavier Yasmin   Zach \r\n     1      3     11 \r\n\r\n#standard quotas\r\n(5*hours)/ sum(hours)\r\n\r\n   Xavier    Yasmin      Zach \r\n0.3333333 1.0000000 3.6666667 \r\n\r\n \r\nIf we look at the standard quotas then we should initially give 0 to Xavier, 1 to Yasmin and 3 to Zach. The question then is what to do with the fifth candy. If we allow zeros then that extra one would be given to Zach. If we don’t allow zeros, then that extra one will be given to Xavier.\r\n\r\n\r\nhamilton(hours,5)\r\n\r\nXavier Yasmin   Zach \r\n     1      1      3 \r\n\r\nhamilton0(hours,5)\r\n\r\nXavier Yasmin   Zach \r\n     0      1      4 \r\n\r\n \r\nThis also leads us to an issue with the hamilton() function when we automatically assign 1 to every group. If there are many individuals with small standard quotas, then it may become impossible to apportion.\r\nFor instance, if we had ten gold watches to give out proportionally to sales reps who sold the most cars in a car dealership and the data looked like this:\r\n\r\n\r\ncars.sold <- c(1, 2, 15, 3, 1, 12, 4)\r\nnames(cars.sold) <- c(paste0(\"rep\", 1:7))\r\ncars.sold\r\n\r\nrep1 rep2 rep3 rep4 rep5 rep6 rep7 \r\n   1    2   15    3    1   12    4 \r\n\r\n#standard quotas\r\n(cars.sold*10) / sum(cars.sold)\r\n\r\n     rep1      rep2      rep3      rep4      rep5      rep6      rep7 \r\n0.2631579 0.5263158 3.9473684 0.7894737 0.2631579 3.1578947 1.0526316 \r\n\r\n \r\nUsing Hamilton’s method everyone would get at least 1 watch accounting for 7 of the watches, but rep3 and rep6 should each get 3 watches, making 11 to be allocated when there are only 10. Clearly this does not work. If it is possible to allocate zero watches to individuals, then we can apportion. Notice that the standard hamilton function reports the error.\r\n\r\n\r\nhamilton(cars.sold, 10)\r\n\r\nError in hamilton(cars.sold, 10): not possible to apportion\r\n\r\nhamilton0(cars.sold, 10)\r\n\r\nrep1 rep2 rep3 rep4 rep5 rep6 rep7 \r\n   0    1    4    1    0    3    1 \r\n\r\n \r\nAnother note is that the hamilton() function won’t work in situations where 1) the values in the input vector are too small, 2) the number of seats to be allocated are either too small.\r\nFor example, let’s say we had the following vector and wanted to allocate 3 seats:\r\n\r\n\r\nz <- c(1,3,11)\r\nnames(z) <- LETTERS[1:3]\r\nz\r\n\r\n A  B  C \r\n 1  3 11 \r\n\r\nz1 <- sum(z) / 3 #standard divisor\r\nz1\r\n\r\n[1] 5\r\n\r\nz2 <- z/z1 #standard quota\r\nz2\r\n\r\n  A   B   C \r\n0.2 0.6 2.2 \r\n\r\n \r\nThe above cannot work as the function will try to allocate 1 seat to each of A and B and then 2 to C. The minimum number of seats that can be allocated are 4.\r\n \r\n \r\nLowndes’ Method\r\nIn 1822, Rep. Lowndes of South Carolina proposed an alternative method to Hamilton’s. The main tenet of this approach was to give a greater emphasis to the smaller states. Consequently, this approach never received much support and did not become law.\r\nThe general logic is as follows:\r\nchoose the number of seats available\r\ncalculate the standard divisor (this is equal to the total population / number of seats)\r\ncalculate the standard quota (this is equal to each state population / standard divisor)\r\nassign each state initially its lower quota (this is the rounded down value of each standard quota)\r\nlist the number of persons per already assigned representative\r\nassign extra seats to those states with the highest number of persons per assigned representative until all seats are filled\r\nThis function also accounts for the zero seats issue by automatically assigning at least one representative per state.\r\nHere is an example of Lowndes’ approach from the 1820 census with 213 seats to assign. I wrote a function lowndes() to complete this task:\r\n\r\n\r\nusa1820\r\n\r\n      New York   Pennsylvania       Virginia           Ohio \r\n       1368775        1049313         895303         581434 \r\nNorth Carolina  Massachusetts       Kentucky South Carolina \r\n        556821         523287         513623         399351 \r\n      Tennesse       Maryland          Maine        Georgia \r\n        390769         364389         298335         281126 \r\n   Connecticut     New Jersey  New Hampshire        Vermont \r\n        275208         274551         244161         235764 \r\n       Indiana      Louisiana        Alabama   Rhode Island \r\n        147102         125779         111147          83038 \r\n      Delaware       Missouri    Mississippi       Illinois \r\n         70943          62496          62320          54843 \r\n\r\nlowndes(usa1820, 213)\r\n\r\n      New York   Pennsylvania       Virginia           Ohio \r\n            32             24             21             13 \r\nNorth Carolina  Massachusetts       Kentucky South Carolina \r\n            13             12             12              9 \r\n      Tennesse       Maryland          Maine        Georgia \r\n             9              8              7              7 \r\n   Connecticut     New Jersey  New Hampshire        Vermont \r\n             7              7              6              6 \r\n       Indiana      Louisiana        Alabama   Rhode Island \r\n             4              3              3              2 \r\n      Delaware       Missouri    Mississippi       Illinois \r\n             2              2              2              2 \r\n\r\n \r\nFor comparison, here is how Lowndes’ approach differs from Hamilton’s for the 120 seats to be assigned in 1790.\r\n\r\n\r\nhamilton(usa1790, 120)\r\n\r\nVA MA PA NC NY MD CT SC NJ NH VT GA KY RI DE \r\n21 16 14 12 11  9  8  7  6  5  3  2  2  2  2 \r\n\r\nlowndes(usa1790, 120)\r\n\r\nVA MA PA NC NY MD CT SC NJ NH VT GA KY RI DE \r\n20 15 14 11 11  9  8  7  6  5  3  3  3  3  2 \r\n\r\n \r\nAs is clearly evident, Lowndes’ method disporportionately favors the smaller states.\r\n \r\n\r\nDivisor Methods\r\nThere are a large number of divisor methods. The general logic is as follows:\r\nchoose the house size (number of seats)\r\nfind a number (the divisor) such that when all the population sizes of every state are divided by this divisor and these numbers are rounded, that they sum to the number of seats.\r\nThe different divisor methods differ in how they round numbers. Further, there is rarely one unique value of the divisor that satisfies this equation, but all values that do for each method will lead to the same apportionment.\r\nAlthough divisor methods are free from paradoxes that the Hamilton Method suffers, they can violate another rule of apportionment the quota rule - that the number of seats allocated should be no higher than the rounded up value of the standard quota and no lower than the rounded down value of this number.\r\n \r\nSome of these divisor apportionment methods are:\r\nJefferson’s Method, aka Greatest Divisor Method, de Hondt’s Method - (used in House from 1791-1830)\r\nAdams’ Method, aka Smallest Divisors Method, - (considered by Congress but not used)\r\nWebster’s Method, aka Webster-Willcox Method, Major Fractions Method, Sainte-Lague Method, - (used in House in 1840, 1910 and 1930)\r\nDean’s Method, aka Harmonic Mean Method\r\nHuntington-Hill Method, aka Geometric Mean Method, Equal Proportions Method, - (used in House from 1940 to present)\r\nThe package currently contains functions called jefferson, jefferson0 and adams that can perform those particular methods.\r\n \r\n \r\nJefferson’s Method\r\nIn 1792, ten days after George Washington vetoed the bill to use the Hamilton Method, Congress passed the method of apportionment suggested by Thomas Jefferson. They also decided to reduce the number of seats available from 120 to 105 to avoid the issue of having representatives representing too few people (i.e. fewer than 30,000).\r\nJefferson’s Method works as follows:\r\nFind the initial divisor - this is equal to the sum of state populations divided by the number of seats available.\r\nDetermine if the sum of the rounded down standard quotas produced by dividing state populations by the divisor are equal to the number of seats available.\r\nIf they are not, then through trial and error, find a divisor such that the sum of the rounded down standard quotas produced by dividing state populations by the divisor are equal to the number of seats available.\r\nIf a state’s lower quota is equal to zero, assign them one representative seat regardless (the jefferson function automatically does this at present).\r\nFor example, here is the 1790 US state populations:\r\n\r\n\r\nusa1790\r\n\r\n    VA     MA     PA     NC     NY     MD     CT     SC     NJ     NH \r\n630560 475327 432879 353523 331589 278514 236841 206236 179570 141822 \r\n    VT     GA     KY     RI     DE \r\n 85533  70835  68705  68446  55540 \r\n\r\ndivisor <- sum(usa1790) / 105\r\ninitial.divisor <- floor(divisor)\r\ninitial.divisor\r\n\r\n[1] 34437\r\n\r\nfirst.sum <- sum(floor(usa1790/initial.divisor))\r\nfirst.sum\r\n\r\n[1] 97\r\n\r\nfirst.sum == 105\r\n\r\n[1] FALSE\r\n\r\n \r\nThe best strategy here as we have too few seats is to decrease the value of the divisor to try and find a solution. This is possible and is what the jefferson function does:\r\n\r\n\r\njefferson(usa1790, 105)\r\n\r\nCT DE GA KY MA MD NC NH NJ NY PA RI SC VA VT \r\n 7  1  2  2 14  8 10  4  5 10 13  2  6 19  2 \r\n\r\nrev(sort(jefferson(usa1790, 105)))\r\n\r\nVA MA PA NY NC MD CT SC NJ NH VT RI KY GA DE \r\n19 14 13 10 10  8  7  6  5  4  2  2  2  2  1 \r\n\r\n \r\n… and to compare this with what Hamilton’s Method would have produced:\r\n\r\n\r\nhamilton(usa1790, 105)\r\n\r\nVA MA PA NC NY MD CT SC NJ NH VT GA KY RI DE \r\n18 14 13 10 10  8  7  6  5  4  2  2  2  2  2 \r\n\r\n \r\nIf Hamilton’s had been used instead then 13 of the 15 states would have received the same number of Representatives. Unsurprisingly, Virginia were the beneficiaries under Jefferson’s method with poor Delaware losing a seat.\r\nThis method was used by the House from 1792-1842 when it was considered that it was too favorable towards the larger States and a different method was needed.\r\n \r\nHere is how the Jefferson Method determined the allocation of seats for the 1832 elections to the House of Representatives with 240 seats being available. I have sorted the allocations in descending order.\r\n\r\n\r\nusa1832\r\n\r\n     NY      PA      VA      OH      NC      TN      KY      MA \r\n1918578 1348072 1023503  937901  639747  625263  621832  610408 \r\n     SC      GA      MD      ME      IN      NJ      CT      VT \r\n 455025  429811  405843  399454  343031  319922  297665  280657 \r\n     NH      AL      LA      IL      MO      MS      RI      DE \r\n 269326  262508  171904  157147  130419  110358   97194   75432 \r\n\r\nrev(sort(jefferson(usa1832, 240)))\r\n\r\nNY PA VA OH TN NC KY MA SC GA ME MD IN NJ CT VT NH AL LA IL RI MS MO \r\n40 28 21 19 13 13 13 12  9  9  8  8  7  6  6  5  5  5  3  3  2  2  2 \r\nDE \r\n 1 \r\n\r\n \r\nQuota Rule Violations\r\nAlthough divisor methods do not have paradoxes associated with them, they can violate the quota rule which states that the number of seats given to each state should be at most the ceiling value of their standard quota and at least the floor value of their standard quota. The 1832 election above is interesting from this point of view because it actually breaks the quota rule. New York is allocated 40 seats, yet this is the standard quota for each state:\r\n\r\n\r\n(240*usa1832)/sum(usa1832)\r\n\r\n       NY        PA        VA        OH        NC        TN        KY \r\n38.593472 27.117365 20.588444 18.866502 12.868936 12.577581 12.508564 \r\n       MA        SC        GA        MD        ME        IN        NJ \r\n12.278763  9.153131  8.645934  8.163802  8.035283  6.900297  6.435444 \r\n       CT        VT        NH        AL        LA        IL        MO \r\n 5.987729  5.645602  5.417672  5.280523  3.457963  3.161116  2.623465 \r\n       MS        RI        DE \r\n 2.219925  1.955122  1.517365 \r\n\r\n \r\nNY should have received only 38 or 39 seats to satisfy the quota rule, but instead it received 40.\r\nInterestingly, according to Balinski and Young’s Impossibility Theorem, it is not possible to come up with a perfect apportionment system - there will always be quota rule violations or paradoxes, but not both, with any system.\r\n \r\nThe quota rule violation can also be understood with this example of a fictitious country called Parador which has 5 states and needs to allocate 250 seats to its House. Below I will show the populations of each state, the standard, lower and upper quotas and the results of Jefferson’s Method using the built-in dataset parador.\r\n\r\n\r\nstate = names(parador)\r\npopn = parador\r\nst.q = (250*parador)/sum(parador)\r\nlow.q = floor(st.q)\r\nupp.q = ceiling(st.q)\r\nseats = jefferson(parador, 250)\r\n\r\ndata.frame(state, popn, st.q, low.q, upp.q, seats)\r\n\r\n  state    popn   st.q low.q upp.q seats\r\nA     A 1646000  32.92    32    33    33\r\nB     B 6936000 138.72   138   139   140\r\nC     C  154000   3.08     3     4     3\r\nD     D 2091000  41.82    41    42    42\r\nE     E  685000  13.70    13    14    13\r\nF     F  988000  19.76    19    20    19\r\n\r\n \r\nAs can be seen, state B actually garners one extra seat that it should not do so according to the quota rule. One of the major criticisms of the Jefferson Method is that it overly favors large states.\r\n \r\nNotes about using the Jefferson Function\r\nThere are two important issues to consider as regards the Jefferson Method function.\r\n1. All the divisor methods utilize a ‘trial-and-error’ method to find a divisor that will correctly apportion the seats to the states based on relative population size. For the jefferson() function to work, there must therefore be a divisor that meets this criteria.\r\nThe way I have programmed the jefferson() function to search for this divisor is to begin with the initial divisor and then try smaller and smaller divisors (initial divisors are too large). The user can adjust the increments by which the function searches for these divisors. The default is set to k=1 which means that the function will search for divisors by decreasing integer values one by one. This works fine for examples such as those above when using large population sizes. However, this might not be appropriate when using smaller initial values in groups. In this case, it may be better to set k to a smaller value such that the function will search for divisors between integers.\r\n \r\nHere is an example:\r\n\r\nA teacher has 10 gold stars to divide between 6 students who are taking an exam. The teachers says they will award the stars based on how many questions each student gets correct.\r\n\r\n\r\n\r\nanswers <- c(1,4,5,14,16,3)\r\nnames(answers) <- paste0(\"Student \", LETTERS[1:6])\r\nanswers\r\n\r\nStudent A Student B Student C Student D Student E Student F \r\n        1         4         5        14        16         3 \r\n\r\n(10*answers)/sum(answers) #standard quotas\r\n\r\nStudent A Student B Student C Student D Student E Student F \r\n0.2325581 0.9302326 1.1627907 3.2558140 3.7209302 0.6976744 \r\n\r\n \r\nLet’s try and apply the jefferson function with the default parameter setting for k=1:\r\n\r\n\r\nError in jefferson(answers, 10): have not found an appropriate divisor - try reducing 'k'\r\n\r\n \r\nThis tells us that it was not able to find an appropriate divisor and that we should try reducing our value of k.\r\n\r\n\r\njefferson(answers, 10, k=0.1)\r\n\r\nStudent A Student B Student C Student D Student E Student F \r\n        1         1         1         3         3         1 \r\n\r\n \r\nNow, we are able to find a solution. My suggestion for adjusting k is to gradually decrease it by one decimal place at a time.\r\nHere is another example.\r\n\r\nIn the country of Tecala, there are four states that wish to elect 160 seats to their house. The population of each state was 3.31, 2.67, 1.33 & 0.69 (in millions).\r\n\r\n \r\n\r\n\r\npopn <- c(3.31, 2.67, 1.33, 0.69)\r\nnames(popn)<- c(\"Apure\", \"Barinas\", \"Carabobo\", \"Dolores\")\r\npopn\r\n\r\n   Apure  Barinas Carabobo  Dolores \r\n    3.31     2.67     1.33     0.69 \r\n\r\njefferson(popn, n=160) # 160 seats\r\n\r\nError in jefferson(popn, n = 160): have not found an appropriate divisor - try reducing 'k'\r\n\r\njefferson(popn, n=160, k=0.1) # 160 seats\r\n\r\nError in jefferson(popn, n = 160, k = 0.1): have not found an appropriate divisor - try reducing 'k'\r\n\r\njefferson(popn, n=160, k=0.01) # 160 seats\r\n\r\nError in jefferson(popn, n = 160, k = 0.01): have not found an appropriate divisor - try reducing 'k'\r\n\r\njefferson(popn, n=160, k=0.001) # 160 seats\r\n\r\nError in jefferson(popn, n = 160, k = 0.001): have not found an appropriate divisor - try reducing 'k'\r\n\r\njefferson(popn, n=160, k=0.0001) # 160 seats\r\n\r\n   Apure  Barinas Carabobo  Dolores \r\n      67       54       26       13 \r\n\r\n \r\n2. Allowing zero apportions. It is possible to allow for zero apportions using jefferson()’s sister function jefferson0().\r\nFor instance, if the teacher in the example above did not feel that all students should get gold stars but only those who were deserving should get them, then we could allocate using jefferson0():\r\n\r\n\r\njefferson0(answers, 10, k=0.1)\r\n\r\nStudent A Student B Student C Student D Student E Student F \r\n        0         1         1         4         4         0 \r\n\r\njefferson(answers, 10, k=0.1)\r\n\r\nStudent A Student B Student C Student D Student E Student F \r\n        1         1         1         3         3         1 \r\n\r\n \r\n\r\nAdams’ Method\r\nIn 1832, former president John Quincy Adams essentially suggested an inverse method to Jefferson’s Method. This method was considered but never passed into law. Essentially, this method operates identically to Jefferson’s except that the rounding method used to determine representatives from the standard quota is a ceiling rounding (i.e. round the standard quota up). This has the result of overly benefiting the smaller states and hence why it didn’t receive much support.\r\nHere is an example for the adams() function for the 1832 House with a size of 240 seats. The function is compared to the Hamilton and Jefferson Methods.\r\n\r\n\r\nusa1832\r\n\r\n     NY      PA      VA      OH      NC      TN      KY      MA \r\n1918578 1348072 1023503  937901  639747  625263  621832  610408 \r\n     SC      GA      MD      ME      IN      NJ      CT      VT \r\n 455025  429811  405843  399454  343031  319922  297665  280657 \r\n     NH      AL      LA      IL      MO      MS      RI      DE \r\n 269326  262508  171904  157147  130419  110358   97194   75432 \r\n\r\nadams(usa1832, 240)\r\n\r\nNY PA VA OH NC TN KY MA SC GA MD ME IN NJ CT VT NH AL LA IL MO MS RI \r\n37 26 20 18 13 12 12 12  9  9  8  8  7  7  6  6  6  6  4  4  3  3  2 \r\nDE \r\n 2 \r\n\r\nrev(sort(jefferson(usa1832, 240))) #function doesn't automatically sort currently\r\n\r\nNY PA VA OH TN NC KY MA SC GA ME MD IN NJ CT VT NH AL LA IL RI MS MO \r\n40 28 21 19 13 13 13 12  9  9  8  8  7  6  6  5  5  5  3  3  2  2  2 \r\nDE \r\n 1 \r\n\r\nhamilton(usa1832, 240)\r\n\r\nNY PA VA OH NC TN KY MA SC GA MD ME IN NJ CT VT NH AL LA IL MO MS RI \r\n39 27 21 19 13 13 12 12  9  9  8  8  7  6  6  6  5  5  3  3  3  2  2 \r\nDE \r\n 2 \r\n\r\n \r\nNotes about using the Adams Function\r\nAs with the jefferson() and jefferson0() functions, the adams() function will automatically stop and return an error message if it cannot find an appropriate divisor. It will encourage the user to use a smaller incremental for the ‘trial-and-error’ process of finding a divisor.\r\nFor example, say we had collected data on how many workers attended an office on each day of the week and we wanted to create a chart such as a waffle chart to visually represent this. However, we wanted a nice rectangular chart that requires 100 percentage points to be allocated. We could use the adams() function.\r\n\r\n\r\nset.seed(111)\r\nworkers <- round(runif(7, 1, 1000))\r\nnames(workers) <- c(\"Mon\", \"Tues\", \"Weds\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\")\r\nworkers\r\n\r\n  Mon  Tues  Weds Thurs   Fri   Sat   Sun \r\n  593   727   371   515   378   419    12 \r\n\r\nadams(workers)  #defaults are n=100, k=1\r\n\r\nError in adams(workers): have not found an appropriate divisor - try reducing 'k'\r\n\r\nadams(workers, k=.1)\r\n\r\n  Mon  Tues  Weds Thurs   Fri   Sat   Sun \r\n   19    24    12    17    13    14     1 \r\n\r\nwaffle::waffle(adams(workers, k=.1), rows=5)\r\n\r\n\r\n \r\nLike the Lowndes Method above, the Adams Method uses a rounding up method, so it is not possible to apportion zeros to groups.\r\n \r\n \r\nWebster’s Method\r\nIn the 1830s another divisor method was suggested by Senator Daniel Webster. Webster’s method can be summarized as follows:\r\nchoose the number of seats available\r\ncalculate the standard divisor (this is equal to the total population / number of seats)\r\ncalculate the standard quota (this is equal to each state population / standard divisor)\r\ncalculate the arithmetic mean of the lower bound (LQ) and upper bound (UQ) of the standard quota of each state\r\nassign a state its upper quota number of states if the standard quota is greater than the arithmetic mean of the LQ and UQ\r\nassign a state its lower quota number of states if the standard quota is less than or equal to the arithmetic mean of the LQ and UQ\r\ncheck if the total number of seats assigned are equal to the number of seats desired.\r\nif the total number of seats are not equal to the number of seats desired, find a divisor that solves this.\r\n \r\nThis method was used in the 1840, 1910 and 1930 apportionments.\r\nI will illustrate Webster’s method using population data from the 1832 apportionment to find 240 seats:\r\n\r\n\r\nusa1832\r\n\r\n     NY      PA      VA      OH      NC      TN      KY      MA \r\n1918578 1348072 1023503  937901  639747  625263  621832  610408 \r\n     SC      GA      MD      ME      IN      NJ      CT      VT \r\n 455025  429811  405843  399454  343031  319922  297665  280657 \r\n     NH      AL      LA      IL      MO      MS      RI      DE \r\n 269326  262508  171904  157147  130419  110358   97194   75432 \r\n\r\ndivisor1832 <- sum(usa1832)/240\r\nstq1832 <- usa1832/divisor1832\r\nwebster.values <- ifelse(stq1832%%1 > 0.5, ceiling(stq1832), floor(stq1832))\r\ncbind(stq1832, webster.values)\r\n\r\n     stq1832 webster.values\r\nNY 38.593472             39\r\nPA 27.117365             27\r\nVA 20.588444             21\r\nOH 18.866502             19\r\nNC 12.868936             13\r\nTN 12.577581             13\r\nKY 12.508564             13\r\nMA 12.278763             12\r\nSC  9.153131              9\r\nGA  8.645934              9\r\nMD  8.163802              8\r\nME  8.035283              8\r\nIN  6.900297              7\r\nNJ  6.435444              6\r\nCT  5.987729              6\r\nVT  5.645602              6\r\nNH  5.417672              5\r\nAL  5.280523              5\r\nLA  3.457963              3\r\nIL  3.161116              3\r\nMO  2.623465              3\r\nMS  2.219925              2\r\nRI  1.955122              2\r\nDE  1.517365              2\r\n\r\nsum(webster.values)\r\n\r\n[1] 241\r\n\r\n \r\nHere, the number of seats produced by this method are 241, which is one too many for the desired 240 seats. The Webster method proceeds by trying different divisors until a solution is found. This is done by the webster() function in apportR.\r\n\r\n\r\nwebster(usa1832,240)\r\n\r\nNY PA VA OH NC TN KY MA SC GA MD ME IN NJ CT VT NH AL LA IL MO MS RI \r\n39 27 21 19 13 13 12 12  9  9  8  8  7  6  6  6  5  5  3  3  3  2  2 \r\nDE \r\n 2 \r\n\r\n \r\nThis shows that the unlucky state was Kentucky who had to give up a seat. Unsurprisingly, the standard quota of Kentucky had the fractional closest to 0.5.\r\n \r\nDean’s Method\r\nDean’s method was devised by Prof James Dean, a professor of astronomy and mathematics at Dartmouth and University of Vermont. It operates in a very similar fashion to Webster’s method, but instead of rounding based on the arithmetic mean, the harmonic mean is used.\r\n \r\n\r\n\r\nharmonic1832 <- 2 / ( (1/floor(stq1832)) + (1/ceiling(stq1832)) )\r\ndean.values <- ifelse(stq1832 > harmonic1832, ceiling(stq1832), floor(stq1832))\r\ncbind(stq1832, harmonic1832, dean.values)\r\n\r\n     stq1832 harmonic1832 dean.values\r\nNY 38.593472    38.493506          39\r\nPA 27.117365    27.490909          27\r\nVA 20.588444    20.487805          21\r\nOH 18.866502    18.486486          19\r\nNC 12.868936    12.480000          13\r\nTN 12.577581    12.480000          13\r\nKY 12.508564    12.480000          13\r\nMA 12.278763    12.480000          12\r\nSC  9.153131     9.473684           9\r\nGA  8.645934     8.470588           9\r\nMD  8.163802     8.470588           8\r\nME  8.035283     8.470588           8\r\nIN  6.900297     6.461538           7\r\nNJ  6.435444     6.461538           6\r\nCT  5.987729     5.454545           6\r\nVT  5.645602     5.454545           6\r\nNH  5.417672     5.454545           5\r\nAL  5.280523     5.454545           5\r\nLA  3.457963     3.428571           4\r\nIL  3.161116     3.428571           3\r\nMO  2.623465     2.400000           3\r\nMS  2.219925     2.400000           2\r\nRI  1.955122     1.333333           2\r\nDE  1.517365     1.333333           2\r\n\r\nsum(dean.values)\r\n\r\n[1] 242\r\n\r\n \r\nThis time the initial value is 2 seats too many. The dean() function will find a solution if possible.\r\n\r\n\r\ndean(usa1832, 240)\r\n\r\nNY PA VA OH NC TN KY MA SC GA MD ME IN NJ CT VT NH AL LA IL MO MS RI \r\n38 27 21 19 13 13 12 12  9  9  8  8  7  6  6  6  5  5  4  3  3  2  2 \r\nDE \r\n 2 \r\n\r\n \r\nDean’s method was never used by Congress. One issue with Dean’s is that it does have bias towards smaller states. For instance, in the example above, NY’s standard quota is 38.59 yet it only gets 38 seats. Alternatively, Louisiana’s standard quota is only 3.46 yet it gets 4 seats. This is because the harmonic mean approaches the arithmetic mean at larger numbers, meaning for smaller numbers, there is a greater possibility of rounding upwards.\r\nBecause finding a divisor that works can be computationally time consuming, we can speed up the process by skipping certain divisors. Setting the parameter k to be a higher value, e.g. 20, will speed up the function considerably. Obviously, we may miss a divisor, in which case it would return an error message and advise adjusting k. For situations where the named vector has much smaller values initially, the user may actually want to reduce k.\r\n\r\n\r\ndean(usa1832, 240,20)\r\n\r\nNY PA VA OH NC TN KY MA SC GA MD ME IN NJ CT VT NH AL LA IL MO MS RI \r\n38 27 21 19 13 13 12 12  9  9  8  8  7  6  6  6  5  5  4  3  3  2  2 \r\nDE \r\n 2 \r\n\r\n \r\n \r\nHuntington-Hill’s Method\r\nHuntington-Hill’s method was initially proposed in 1911 by Joseph A. Hill, Chief Statistician of the Bureau of the Census and later added to by Prof Edward Huntington, professor of mechanics & mathematics at Harvard. This operates in a very similar fashion to Webster’s and Dean’s method, but use the geometric mean to assign rounding.\r\nThis method has been used by the House since 1940 and is still the currently used method. Below is an example of applying this method to the 1832 data.\r\n\r\n\r\ngmean1832 <- (sqrt(floor(stq1832) * ceiling(stq1832)))\r\nhhill.values <- ifelse(stq1832 > gmean1832, ceiling(stq1832), floor(stq1832))\r\ncbind(stq1832, gmean1832, hhill.values)\r\n\r\n     stq1832 gmean1832 hhill.values\r\nNY 38.593472 38.496753           39\r\nPA 27.117365 27.495454           27\r\nVA 20.588444 20.493902           21\r\nOH 18.866502 18.493242           19\r\nNC 12.868936 12.489996           13\r\nTN 12.577581 12.489996           13\r\nKY 12.508564 12.489996           13\r\nMA 12.278763 12.489996           12\r\nSC  9.153131  9.486833            9\r\nGA  8.645934  8.485281            9\r\nMD  8.163802  8.485281            8\r\nME  8.035283  8.485281            8\r\nIN  6.900297  6.480741            7\r\nNJ  6.435444  6.480741            6\r\nCT  5.987729  5.477226            6\r\nVT  5.645602  5.477226            6\r\nNH  5.417672  5.477226            5\r\nAL  5.280523  5.477226            5\r\nLA  3.457963  3.464102            3\r\nIL  3.161116  3.464102            3\r\nMO  2.623465  2.449490            3\r\nMS  2.219925  2.449490            2\r\nRI  1.955122  1.414214            2\r\nDE  1.517365  1.414214            2\r\n\r\nsum(dean.values)\r\n\r\n[1] 242\r\n\r\n \r\nThis method is still somewhat biased to smaller states as the geometric mean is smaller than the arithmetic mean, but this difference is not as pronounced as the harmonic mean used in Dean’s method.\r\n \r\nImportant notes about Webster, Dean and Huntington-Hill methods\r\nIn the case of assigning seats to a parliament, Webster’s method could theoretically apportion 0 seats but Dean’s method and Huntington-Hill’s methods will not - they will always assign at least one seat. This can be understood by demonstrating what happens with Nevada in the 1940 census when the House had 435 seats.\r\n\r\n\r\nusa1940\r\n\r\n      NY       PA       IL       OH       CA       TX       MI \r\n13479142  9900180  7897241  6907612  6907387  6414824  5256106 \r\n      MA       NJ       MO       NC       IN       WI       GA \r\n 4316721  4160165  3784664  3571623  3427796  3137587  3123723 \r\n      TN       KY       AL       MN       VA       IA       LA \r\n 2915841  2845627  2832961  2792300  2677773  2538268  2363880 \r\n      OK       MS       AR       WV       SC       FL       MD \r\n 2336434  2183796  1949387  1901974  1899804  1897414  1821244 \r\n      KS       WA       CT       NE       CO       OR       ME \r\n 1801028  1736191  1709242  1315834  1123296  1089684   847226 \r\n      RI       SD       ND       MT       UT       NM       ID \r\n  713346   642961   641935   559456   550310   531818   524873 \r\n      AZ       NH       VT       DE       WY       NV \r\n  499261   491524   359231   266505   250742   110247 \r\n\r\n \r\nLet’s look at the standard quotas of Delaware, Wyoming and Nevada:\r\n\r\n\r\ntail(usa1940 / (sum(usa1940)/435), 3)\r\n\r\n       DE        WY        NV \r\n0.8849176 0.8325773 0.3660701 \r\n\r\n \r\nNevada has the lowest standard quota and the only one under 0.5. Now let’s calculate the lower quota and upper quota and the arithmetic mean, harmonic mean and geometric means of these two numbers:\r\n\r\n\r\nx1940 <- data.frame(stq = tail(usa1940 / (sum(usa1940)/435), 3))\r\nx1940$lq <- floor(x1940$stq)\r\nx1940$uq <- ceiling(x1940$stq)\r\nx1940$arith <- mean(c(floor(x1940$stq), ceiling(x1940$stq)))\r\nx1940$harmon <-  2 / ( (1/floor(x1940$stq)) + (1/ceiling(x1940$stq)) )\r\nx1940$geom <- (sqrt(floor(x1940$stq) * ceiling(x1940$stq)))\r\n\r\nx1940\r\n\r\n         stq lq uq arith harmon geom\r\nDE 0.8849176  0  1   0.5      0    0\r\nWY 0.8325773  0  1   0.5      0    0\r\nNV 0.3660701  0  1   0.5      0    0\r\n\r\n \r\nThe harmonic and geometric means of 0 and 1 are 0 and therefore the standard quota of all states is always going to be higher than this meaning that these states are going to be assigned at least 1 seat automatically. With Webster’s method that uses the arithmetic mean for rounding, it is possible for the standard quota of any state to be lower than the arithmetic mean (0.5), resulting in a state being allocated 0 seats.\r\nObviously, this fails the common sense test and Webster’s method has to be altered accordingly to be able to assign a seat to a state in this situation. All three functions, webster(), dean() and hhill() will automatically always assign at least 1 seat.\r\nFor example, here are the results of the three methods compared to one another:\r\n\r\n\r\nwebster1940<-webster(usa1940, 435)\r\nhhill1940<-hhill(usa1940, 435)\r\ndean1940<-dean(usa1940, 435)\r\n\r\nwebster1940 <- webster1940[names(usa1940)]\r\nhhill1940 <- hhill1940[names(usa1940)]\r\ndean1940 <- dean1940[names(usa1940)]\r\n\r\ndata.frame(webster=webster1940, hhill=hhill1940, dean=dean1940)\r\n\r\n   webster hhill dean\r\nNY      45    45   45\r\nPA      33    33   33\r\nIL      26    26   26\r\nOH      23    23   23\r\nCA      23    23   23\r\nTX      21    21   21\r\nMI      18    17   17\r\nMA      14    14   14\r\nNJ      14    14   14\r\nMO      13    13   13\r\nNC      12    12   12\r\nIN      11    11   11\r\nWI      10    10   10\r\nGA      10    10   10\r\nTN      10    10   10\r\nKY       9     9    9\r\nAL       9     9    9\r\nMN       9     9    9\r\nVA       9     9    9\r\nIA       8     8    8\r\nLA       8     8    8\r\nOK       8     8    8\r\nMS       7     7    7\r\nAR       6     7    7\r\nWV       6     6    6\r\nSC       6     6    6\r\nFL       6     6    6\r\nMD       6     6    6\r\nKS       6     6    6\r\nWA       6     6    6\r\nCT       6     6    6\r\nNE       4     4    4\r\nCO       4     4    4\r\nOR       4     4    4\r\nME       3     3    3\r\nRI       2     2    2\r\nSD       2     2    2\r\nND       2     2    2\r\nMT       2     2    2\r\nUT       2     2    2\r\nNM       2     2    2\r\nID       2     2    2\r\nAZ       2     2    2\r\nNH       2     2    2\r\nVT       1     1    1\r\nDE       1     1    1\r\nWY       1     1    1\r\nNV       1     1    1\r\n\r\n \r\nAs can be seen from the table above, the Webster Method allocated 18 seats to Michigan and 6 to Arkansas, whereas Huntington-Hill’s method (and Dean’s Method) gave 7 to Arkansas and 17 to Michigan.\r\nUnsurprisingly, a Democratic representative from Arkansas sponsored a bill to use Huntington-Hill’s method to apportion the House. This was opposed by all the Republican representatives and the Democrats from Michigan. Nevertheless, this bill passed and Huntington-Hill’s method has been used ever since, and the number of seats has remained at 435.\r\n \r\nAssigning Zero Apportions\r\nIn other situations, it may be necessary to apportion zero to one subject in the final apportions. This could happen when using the Webster method and a subject’s standard quota is beneath 0.5, but it could also occur when using Dean’s method or Huntington-Hill’s method if the standard quota is also 0 (i.e. if the individual subject in the named vector was initially zero).\r\nFor example, if special allowances had not been made in the 1940 apportionment, then Nevada would have received 0 seats under Webster’s method as their standard quota was actually below 0.5. This is shown here with the function webster0() which allows for 0 seats to be allocated:\r\n\r\n\r\nwebster0(usa1940, 435)\r\n\r\nNY PA IL OH CA TX MI MA NJ MO NC IN WI GA TN KY AL MN VA IA LA OK MS \r\n45 33 26 23 23 21 18 14 14 13 12 11 10 10 10  9  9  9  9  8  8  8  7 \r\nAR WV SC FL MD KS WA CT NE CO OR ME RI SD ND MT UT NM ID AZ NH VT DE \r\n 7  6  6  6  6  6  6  6  4  4  4  3  2  2  2  2  2  2  2  2  2  1  1 \r\nWY NV \r\n 1  0 \r\n\r\n \r\nNevada’s seat would be given to Arkansas in this situation.\r\n \r\nTo illustrate the possibility of apportioning zeros using the Dean or Huntington-Hill methods, imagine the following scenario.\r\n\r\nIf 10 scholarships were to be given out to subject based on the proportion of students majoring in that subject that year but one of the subjects actually contained zero students.\r\n\r\nComparing each method allowing and not-allowing for zero apportionment:\r\n\r\n\r\nmajs <- c(100, 154, 0, 22, 5)\r\nnames(majs) <- c(\"English\", \"History\", \"Swahili\", \"French\", \"Aeronautics\")\r\nmajs\r\n\r\n    English     History     Swahili      French Aeronautics \r\n        100         154           0          22           5 \r\n\r\nwebster(majs, 10)\r\n\r\n    English     History     Swahili      French Aeronautics \r\n          3           4           1           1           1 \r\n\r\ndean(majs, 10)\r\n\r\n    English     History     Swahili      French Aeronautics \r\n          3           4           1           1           1 \r\n\r\nhhill(majs, 10)\r\n\r\n    English     History     Swahili      French Aeronautics \r\n          3           4           1           1           1 \r\n\r\nwebster0(majs, 10)\r\n\r\n    English     History     Swahili      French Aeronautics \r\n          4           5           0           1           0 \r\n\r\ndean0(majs, 10)\r\n\r\n    English     History     Swahili      French Aeronautics \r\n          3           5           0           1           1 \r\n\r\nhhill0(majs, 10)\r\n\r\n    English     History     Swahili      French Aeronautics \r\n          3           5           0           1           1 \r\n\r\n \r\nClearly, when allowing for zero apportionment, Dean’s Method and Huntington-Hill’s Method only give 0’s when a subject initially contained zeros, whereas Webster’s Method will also apportion zeros when the standard quota is less than 0.5 as is the case here for Aeronautics.\r\nGoing on from this, it’s also clear that the results for Dean’s method and for Huntington-Hill’s method are the same as if the ‘Swahili’ group never existed, i.e.\r\n\r\n\r\nmajs1 <- c(100, 154, 22, 5)\r\nnames(majs1) <- c(\"English\", \"History\", \"French\", \"Aeronautics\")\r\n\r\nmajs1\r\n\r\n    English     History      French Aeronautics \r\n        100         154          22           5 \r\n\r\ndean0(majs1, 10)\r\n\r\n    English     History      French Aeronautics \r\n          3           5           1           1 \r\n\r\nhhill0(majs1, 10)\r\n\r\n    English     History      French Aeronautics \r\n          3           5           1           1 \r\n\r\n \r\n1876 Presidential Election\r\nPerhaps the biggest snafu in US Presidential electoral history occurred as a result of mis-apportionment.\r\nThe House of Representatives in 1872 was apportioned according to the US census of 1870 and the House size was 292. Even though the legal method in place at the time was Hamilton’s method, the apportionment that occurred had the same results as Dean’s Method (even if it’s not clear that Dean’s Method was actually used).\r\n\r\n\r\nusa1870\r\n\r\n     NY      PA      OH      IL      MO      IN      MA      KY \r\n4382759 3521951 2665260 2539891 1721295 1680637 1457351 1321011 \r\n     TN      VA      IA      GA      MI      NC      WI      AL \r\n1258520 1225163 1194020 1184109 1184059 1071361 1054670  996992 \r\n     NJ      MS      TX      MD      LA      SC      ME      CA \r\n 906096  827922  818579  780894  726915  705606  626915  560247 \r\n     CT      AR      WV      MN      KS      VT      NH      RI \r\n 537454  484471  442014  439706  364399  330551  318300  217353 \r\n     FL      DE      NE      OR      NV \r\n 187748  125015  122993   90923   42491 \r\n\r\nhamilton(usa1870, 292)\r\n\r\nNY PA OH IL MO IN MA KY TN VA IA GA MI NC WI AL NJ MS TX MD LA SC ME \r\n34 27 20 20 13 13 11 10 10  9  9  9  9  8  8  8  7  6  6  6  6  5  5 \r\nCA CT AR WV MN KS VT NH RI FL DE NE OR NV \r\n 4  4  4  3  3  3  3  2  2  1  1  1  1  1 \r\n\r\ndean(usa1870, 292)\r\n\r\nNY PA OH IL MO IN MA KY TN VA IA GA MI NC WI AL NJ MS TX MD LA SC ME \r\n33 27 20 19 13 13 11 10 10  9  9  9  9  8  8  8  7  6  6  6  6  5  5 \r\nCA CT AR WV MN KS VT NH RI FL DE NE OR NV \r\n 4  4  4  3  3  3  3  3  2  2  1  1  1  1 \r\n\r\nyou will notice that four states ended up with different number of representatives than they should have received under the legally constituted Hamilton’s Method. New York and Illinois received one seat too few whilst New Hampshire and Florida received one extra seat each.\r\nThis ended up being historically very significant. In the Presidential election of 1876, Samuel Tilden won the state of New York and received 35 electoral votes. He would have received 36 if the Hamilton apportionment had been used. His opponent, Rutherford B. Hayes, won the other three states and had the Hamilton Method been applied would have received one fewer electoral vote in both Florida and New Hampshire (he would also have actually gained one electoral college vote in Illinois). In the election, Hayes beat Tilden in the Electoral College by a vote of 185 to 184. So, if the correct apportionment had actually been followed, the vote would have been 185 to 184 in favor of Tilden!\r\n \r\nThis continues to have relevance for today. Had Jefferson’s Method been used to apportion the House prior to the 2000 US Presidential election, then Gore would have defeated Bush in the electoral college count! Had Hamilton’s method been used then it would have been a tie!\r\n \r\nIf you made it this far in this blog - I’m amazed. I barely could make it this far myself. This was a very strange rabbit hole to go down. I wish I could have just accepted that those waffle charts didn’t quite add up to 100 - but it turns out that I couldn’t let it lie, and I ended up doing all of this. Not sure it was worth it!\r\nReferences and Further Reading\r\nThere are many online resources that describe the apportionment problem, here are some that are worthwhile:\r\nMathematical Association of America\r\nAmerican Mathematical Association\r\nThe Mathematics of Apportionment\r\n",
    "preview": "posts/2020-10-16-apportionment-methods/apportionment-methods_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-03T13:41:28-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-10-17-scoring-doesnt-stop/",
    "title": "Scoring doesn't Stop?",
    "description": "Looking at the trend of high scoring games in the EPL",
    "author": [
      {
        "name": "James Curley",
        "url": "jamescurley.blog"
      }
    ],
    "date": "2020-10-17",
    "categories": [
      "soccer",
      "Rpackage"
    ],
    "contents": "\r\nMy most popular R package is engsoccerdata. It started as a project to collect soccer results for the top four English leagues, but has morphed into a ridiculously large project. We currently have historical soccer league results from England, Scotland, Germany, Holland, Italy, Spain, France, Turkey, USA, Portugal, Belgium, Greece, & South Africa, as well as cup and other competition data. I’m always looking for people who are interested in helping maintain the package. Many thanks to everyone who has helped over the last 6 years - particularly Robert Hickman and Joe Gallagher who have put in lots of work on it.\r\nIn this post, I just want to focus on the recent trend of high scoring in the EPL that has happened during the post-Covid 2020/21 season.\r\n\r\nThe data\r\nIf you don’t have the package, you’ll need to install it. The dataset we want to look at is england.\r\n\r\n\r\nlibrary(devtools)\r\ninstall_github(\"jalapic/engsoccerdata\")\r\n\r\n\r\n\r\n\r\nlibrary(engsoccerdata)\r\nhead(england)\r\n\r\n        Date Season                    home              visitor  FT\r\n1 1888-09-08   1888        Bolton Wanderers         Derby County 3-6\r\n2 1888-09-08   1888                 Everton      Accrington F.C. 2-1\r\n3 1888-09-08   1888       Preston North End              Burnley 5-2\r\n4 1888-09-08   1888              Stoke City West Bromwich Albion 0-2\r\n5 1888-09-08   1888 Wolverhampton Wanderers          Aston Villa 1-1\r\n6 1888-09-15   1888             Aston Villa           Stoke City 5-1\r\n  hgoal vgoal division tier totgoal goaldif result\r\n1     3     6        1    1       9      -3      A\r\n2     2     1        1    1       3       1      H\r\n3     5     2        1    1       7       3      H\r\n4     0     2        1    1       2      -2      A\r\n5     1     1        1    1       2       0      D\r\n6     5     1        1    1       6       4      H\r\n\r\ntail(england)\r\n\r\n             Date Season              home             visitor  FT\r\n200623 2020-03-07   2019   Plymouth Argyle        Macclesfield 3-0\r\n200624 2020-03-07   2019      Salford City       Bradford City 2-0\r\n200625 2020-03-07   2019 Scunthorpe United        Grimsby Town 0-2\r\n200626 2020-03-07   2019      Swindon Town Forest Green Rovers 0-2\r\n200627 2020-03-07   2019           Walsall         Exeter City 3-1\r\n200628 2020-03-10   2019   Carlisle United      Newport County 2-0\r\n       hgoal vgoal division tier totgoal goaldif result\r\n200623     3     0        4    4       3       3      H\r\n200624     2     0        4    4       2       2      H\r\n200625     0     2        4    4       2      -2      A\r\n200626     0     2        4    4       2      -2      A\r\n200627     3     1        4    4       4       2      H\r\n200628     2     0        4    4       2       2      H\r\n\r\n\r\nThe dataset has every soccer result in the top 4 tiers of English football from the 1880/81 season to the 2019/2020 season. What it doesn’t contain are data from the 2020/21 season that is ongoing. However, there is a function to collect that:\r\n\r\n\r\nengland20 <- england_current()\r\n\r\ntail(england20)\r\n\r\n          Date Season              home          visitor  FT hgoal\r\n241 2020-10-17   2020    Mansfield Town    Bradford City 1-3     1\r\n242 2020-10-17   2020    Newport County  Tranmere Rovers 1-0     1\r\n243 2020-10-17   2020         Port Vale     Salford City 1-0     1\r\n244 2020-10-17   2020 Scunthorpe United Cambridge United 0-5     0\r\n245 2020-10-17   2020   Southend United       Cheltenham 0-2     0\r\n246 2020-10-17   2020           Walsall      Exeter City 0-0     0\r\n    vgoal division tier totgoal goaldif result\r\n241     3        4    4       4      -2      A\r\n242     0        4    4       1       1      H\r\n243     0        4    4       1       1      H\r\n244     5        4    4       5      -5      A\r\n245     2        4    4       2      -2      A\r\n246     0        4    4       0       0      D\r\n\r\n\r\nWe can bind these two together:\r\n\r\n\r\ndf <- rbind(england, england20)\r\n\r\n\r\nThe totgoal column gives us the total number of goals in each game. We can do some tidyverse to look at the average number of goals per game season by season:\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\ndf %>%\r\n  group_by(Season,tier) %>%\r\n  summarise(gpg = mean(totgoal)) -> df.sum\r\n\r\n\r\n\r\n\r\nggplot(df.sum, aes(x=Season, y=gpg, color=factor(tier))) +\r\n  geom_line() +\r\n  scale_color_manual(values=c(\"red\", \"blue\", \"darkorange\",\"black\"), name = \"Tier\") +\r\n  theme_minimal() +\r\n  ylab(\"Average Goals Per Game\") +\r\n  xlab(\"Season\") +\r\n  ggtitle(\"Changes in Goals per Game Across Time\")\r\n\r\n\r\n\r\nThe historical decline in scoring over time are well documented. My good friend Ollie Roeder and I have discussed it in some other pieces, e.g. here for Contexts and here for 538. Since tactical changes that came about in the mid 1960s, scoring has been relatively stable across all four tiers of English soccer.\r\nWhat’s happening this season is quite striking. People have obviously noticed that the top tier - the EPL - has had a much higher increase in goals per game. Below are the goal per game over the last 8 years in the top tier:\r\n\r\n\r\n df.sum %>% filter(Season>2012, tier==1)\r\n\r\n# A tibble: 8 x 3\r\n# Groups:   Season [8]\r\n  Season  tier   gpg\r\n   <dbl> <dbl> <dbl>\r\n1   2013     1  2.77\r\n2   2014     1  2.57\r\n3   2015     1  2.7 \r\n4   2016     1  2.8 \r\n5   2017     1  2.68\r\n6   2018     1  2.82\r\n7   2019     1  2.75\r\n8   2020     1  3.72\r\n\r\n\r\nYou can see that scoring has shot up by more than a goal per game in the current season.\r\nTiers 3 and 4 appear to be going along at the same rate. What’s happening with the Championship (tier 2) though ? They’ve gone in the opposite direction. Here are the average goals per game in the last 8 seasons in the second tier:\r\n\r\n\r\ndf.sum %>% filter(Season>2012, tier==2)\r\n\r\n# A tibble: 8 x 3\r\n# Groups:   Season [8]\r\n  Season  tier   gpg\r\n   <dbl> <dbl> <dbl>\r\n1   2013     2  2.60\r\n2   2014     2  2.67\r\n3   2015     2  2.42\r\n4   2016     2  2.61\r\n5   2017     2  2.55\r\n6   2018     2  2.67\r\n7   2019     2  2.63\r\n8   2020     2  2   \r\n\r\n Scoring has dropped by about 0.6 goals per game! Obviously, it’s still relatively early in the season. At the time of writing, only 59 games have been played in the Championship - so small sample sizes etc. But, given all the arguments for the increased rate of scoring in the EPL revolve around crowds being removed - we do not see that pattern in the Championship. I could look at similar patterns in other European leagues using my package, but will leave that for later in the season.\r\n\r\nGames without nil-nils.\r\nAnother way of looking at the increased scoring is to look at how many nil-nil (0-0) games there have been. A few commentators have recently described that it’s been a long run in the EPL without there being a 0-0. Let’s take a look. We can see that there has been no game this season that has had 0 goals:\r\n\r\n\r\nengland20 %>% filter(tier==1) %>% .$totgoal\r\n\r\n [1] 1 3 7 2 1 3 4 2 3 7 7 4 2 6 3 7 1 4 5 1 3 6 7 1 2 4 3 4 4 6 2 4 3\r\n[34] 9 3 7 2 1 6 4 1 5 2 1 2 6\r\n\r\n\r\nLet’s write a bit of code to try and find the last one. I’m going to use data.table’s rleid() to create a new column that counts the number of rows since there was a 0 in totgoal. The data are organized in ascending date order, so this should work. The one issue is that we don’t know which games started before which others if they played on the same day. This probably isn’t going to be a huge deal - so let’s press forward:\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(data.table)\r\n\r\ndf1 <- df %>%\r\n  filter(tier==1) %>%\r\n  group_by(ID = data.table::rleid(totgoal != 0)) %>%\r\n  mutate(count = if_else(totgoal != 0, row_number(), 0L))\r\n\r\ntail(df1[c(1:5,10,14)])\r\n\r\n# A tibble: 6 x 7\r\n  Date      Season home          visitor           FT    totgoal count\r\n  <chr>      <dbl> <chr>         <chr>             <chr>   <dbl> <int>\r\n1 2020-10-~   2020 Manchester C~ Arsenal           1-0         1    57\r\n2 2020-10-~   2020 Newcastle Un~ Manchester United 1-4         5    58\r\n3 2020-10-~   2020 Crystal Pala~ Brighton & Hove ~ 1-1         2    59\r\n4 2020-10-~   2020 Leicester Ci~ Aston Villa       0-1         1    60\r\n5 2020-10-~   2020 Sheffield Un~ Fulham            1-1         2    61\r\n6 2020-10-~   2020 Tottenham Ho~ West Ham United   3-3         6    62\r\n\r\n\r\nWe can see here, that the Newcastle United vs Manchester Untied game on the 17th October 2020 that finished 1-4 was the 58th game without being a 0-0. Technically, it might be e.g. the 57th or 59th, depending on when the last 0-0 game finished, but we get the idea.\r\nThe last 0-0 game in the EPL was Brighton 0-0 Newcastle on 20th July:\r\n\r\n\r\ndf1 %>% filter(count==0) %>% select(1:5,10,14) %>% tail()\r\n\r\n# A tibble: 6 x 8\r\n# Groups:   ID [6]\r\n     ID Date     Season home         visitor       FT    totgoal count\r\n  <int> <chr>     <dbl> <chr>        <chr>         <chr>   <dbl> <int>\r\n1  6222 2020-03~   2019 Wolverhampt~ Brighton & H~ 0-0         0     0\r\n2  6224 2020-06~   2019 Aston Villa  Sheffield Un~ 0-0         0     0\r\n3  6226 2020-06~   2019 Everton      Liverpool     0-0         0     0\r\n4  6228 2020-06~   2019 Leicester C~ Brighton & H~ 0-0         0     0\r\n5  6230 2020-07~   2019 AFC Bournem~ Tottenham Ho~ 0-0         0     0\r\n6  6232 2020-07~   2019 Brighton & ~ Newcastle Un~ 0-0         0     0\r\n\r\n\r\nWe can actually look at what other games took place on that date. (Note, the Date column is a character when imported - so here I’ll just string match).\r\n\r\n\r\ndf1 %>% \r\n  filter(Date==\"2020-07-20\") %>%\r\n   select(1:5,10,14)\r\n\r\n# A tibble: 3 x 8\r\n# Groups:   ID [2]\r\n     ID Date     Season home           visitor     FT    totgoal count\r\n  <int> <chr>     <dbl> <chr>          <chr>       <chr>   <dbl> <int>\r\n1  6232 2020-07~   2019 Brighton & Ho~ Newcastle ~ 0-0         0     0\r\n2  6233 2020-07~   2019 Sheffield Uni~ Everton     0-1         1     1\r\n3  6233 2020-07~   2019 Wolverhampton~ Crystal Pa~ 2-0         2     2\r\n\r\n\r\nIt turns out there were two other games that day, and the Brighton-Newcastle game was actually the first to finish. Therefore, the current streak is 58.\r\nJust for interest, let’s plot a graph of the length of other streaks over time. To do this, we’ll get the count immediately prior to each 0. Then I’ll just plot the Season on the x-axis and the count on the y-axis.\r\nI couldn’t remember the best way to do this in dplyr, so I did it with base r:\r\n\r\n\r\n\r\ndf_no00 <- df1[which((df1$count==0)==T)-1,c(1:5,10,14)]\r\n\r\n# need to add in the current streak \r\ndf_no00 <- rbind(df_no00,tail(df1[c(1:5,10,14)],1)) \r\n\r\nhead(df_no00)\r\n\r\n# A tibble: 6 x 7\r\n  Date      Season home             visitor        FT    totgoal count\r\n  <chr>      <dbl> <chr>            <chr>          <chr>   <dbl> <int>\r\n1 1888-10-~   1888 Preston North E~ West Bromwich~ 3-0         3    35\r\n2 1888-12-~   1888 Burnley          Preston North~ 2-2         4    45\r\n3 1890-01-~   1889 Wolverhampton W~ Derby County   2-1         3   155\r\n4 1890-09-~   1890 Aston Villa      Notts County   3-2         5    32\r\n5 1890-11-~   1890 Blackburn Rovers Everton        2-1         3    50\r\n6 1890-12-~   1890 Sunderland       Everton        1-0         1    32\r\n\r\ntail(df_no00)\r\n\r\n# A tibble: 6 x 7\r\n  Date       Season home            visitor        FT    totgoal count\r\n  <chr>       <dbl> <chr>           <chr>          <chr>   <dbl> <int>\r\n1 2020-03-09   2019 Leicester City  Aston Villa    4-0         4     3\r\n2 2020-06-21   2019 Aston Villa     Chelsea        1-2         3     8\r\n3 2020-06-22   2019 Manchester City Burnley        5-0         5     2\r\n4 2020-07-08   2019 West Ham United Burnley        0-1         1    36\r\n5 2020-07-19   2019 Tottenham Hots~ Leicester City 3-0         3    25\r\n6 2020-10-18   2020 Tottenham Hots~ West Ham Unit~ 3-3         6    62\r\n\r\n\r\nIt seems that the current streak of 58 games is notable in the current era, but less so in the 19th Century! Let’s plot these\r\n\r\n\r\n\r\nggplot(df_no00, aes(x=Season, y = count)) +\r\n  geom_point() +\r\n  theme_minimal()\r\n\r\n\r\n\r\nMy first thoughts are that 58 is quite a high number of games without there being a 0-0, although it looks like it’s not super uncommon. If we were to get to 75 games without a 0-0 then it would be a really historical streak.\r\nThere are so many streaks of a small number of games since a 0-0, that I’m going to remake this figure, but only include streaks of over 25:\r\n\r\n\r\nggplot(df_no00 %>% filter(count>=25), aes(x=Season, y = count)) +\r\n  geom_point(alpha=.5) +\r\n  theme_minimal() +\r\n  ylab(\"Number of games\")+\r\n  ggtitle(\"Games Since a 0-0 draw by Season\")\r\n\r\n\r\n\r\nLet’s just identify when the last pretty long streak happened:\r\n\r\n\r\ndf_no00 %>% filter(count>50) %>% tail()\r\n\r\n# A tibble: 6 x 7\r\n  Date      Season home          visitor           FT    totgoal count\r\n  <chr>      <dbl> <chr>         <chr>             <chr>   <dbl> <int>\r\n1 2017-11-~   2017 Newcastle Un~ Watford           0-3         3    52\r\n2 2018-12-~   2018 Manchester C~ Crystal Palace    2-3         5    53\r\n3 2019-04-~   2018 West Ham Uni~ Leicester City    2-2         4    54\r\n4 2019-12-~   2019 Tottenham Ho~ Burnley           5-0         5    59\r\n5 2020-01-~   2019 Southampton   Wolverhampton Wa~ 2-3         5    51\r\n6 2020-10-~   2020 Tottenham Ho~ West Ham United   3-3         6    62\r\n\r\n\r\nIt does seem that within most EPL seasons we get streaks of about 50 games without a 0-0. Probably it’s just surprising that the current streak is at the beginning of the season when goals per game is also shooting up. If it goes on another dozen or so games, then it will start to be notable!\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-17-scoring-doesnt-stop/scoring-doesnt-stop_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-10-03T13:41:28-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-10-15-langford-sequences/",
    "title": "Langford Sequences",
    "description": "Describing Langford and Skolem sequences and how to search for them in R.",
    "author": [
      {
        "name": "James Curley",
        "url": "jamescurley.blog"
      }
    ],
    "date": "2020-10-16",
    "categories": [
      "puzzles"
    ],
    "contents": "\r\nIn 1958, the mathematician C.D. Langford was watching his son play with blocks. He wrote:\r\n\r\nThere were two of each color, and one day I noticed that he had placed them in a single pile so that between the red pair there was one block, two between the blue pair, and three between the yellow. I then found that by a complete rearrangement I could add a green pair with four between them.\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe general problem was summarized by Hayasaka & Saito (1979) as: “Given \\(2n\\) numbers, two each of the numbers \\(1, 2, …., n\\) to find whether they can be arranged in a row in such a way that the two occurrences of each number \\(k\\) are separated by exactly \\(k\\) other elements of the row.\r\nLangford actually discovered several examples for various different number of blocks. Here are some examples of his sequences:\r\n\r\n\r\n\r\n\r\nWhat one might notice is that all the solutions are for a number of blocks \\(n\\) that are divisible by 4, or one less than a number that is divisible by 4. A bit more explanation for why this is so can be found at Nick Berry’s excellent blog.\r\nAs \\(n\\) increases, there are an increasing number of Langford Sequences. There is only one solution for \\(n=3\\) & \\(n=4\\), 26 solutions for \\(n=7\\), 150 for \\(n=8\\), 17,792 for \\(n=11\\), 108,144 for \\(n=12\\) ….. 46,845,158,056,515,936 for \\(n=24\\) ! Obviously, trying to find all solutions for any given \\(n\\) is computationally intensive!\r\nAlthough Langford sequences can only be found when \\(n\\) is divisible by 4 or when \\(n+1\\) is divisible by 4, there is an alternative type of Langford sequence called a Hooked Langford Sequence which can be found for all values of \\(n\\). A Hooked Langford Sequence is one where another block/letter is inserted in the penultimate position in the sequence.\r\nAt the same time as Langford wrote his piece, a related phenomenon was being independently researched by a Swedish mathematician, Thoralf Skolem, who was trying to form sequences of \\(2n\\) blocks with intervals of \\(0,1,2,3 ... n\\).\r\nMy aim here is to write a small script that can find these sequences. Below is an image of each type of these sequences:\r\n\r\n\r\n\r\n\r\n\r\nLangford’s Sequences\r\nSay we have \\(2*8\\) blocks, let’s represent them with letters. We can then generate a randomly sample sequence of these letters as a starting point. A random shuffle is unlikely to be a Langford sequence. We an check by finding the differences between the positions/indexes of each letter in the vector:\r\n\r\n\r\nx <- rep(LETTERS[1:8],2)\r\nset.seed(1)\r\ny <- sample(x)\r\ny\r\n\r\n [1] \"E\" \"F\" \"A\" \"D\" \"C\" \"B\" \"C\" \"G\" \"F\" \"A\" \"B\" \"G\" \"E\" \"H\" \"D\" \"H\"\r\n\r\n\r\nThis gets the difference (number of letters between) the pairs of A’s:\r\n\r\n\r\nabs(diff(which(y==\"A\")))-1  #6\r\n\r\n[1] 6\r\n\r\n\r\nLikewise for the D’s and the H’s:\r\n\r\n\r\nabs(diff(which(y==\"D\")))-1  #10\r\n\r\n[1] 10\r\n\r\nabs(diff(which(y==\"H\")))-1  #1\r\n\r\n[1] 1\r\n\r\n\r\nWe can get the results for all the individual letters with a loop:\r\n\r\n\r\nres<-NULL\r\nfor(i in 1:8){  res[[i]] <- abs(diff(which(y==LETTERS[i])))-1 }\r\n          \r\nnames(res)<-LETTERS[1:8]\r\nres\r\n\r\n A  B  C  D  E  F  G  H \r\n 6  4  1 10 11  6  3  1 \r\n\r\n\r\nThis is fine for one randomly sampled sequence, but when finding Langford sequences, we might want to check many sequences, so it is better to have a faster function. match() seems to be pretty fast for this purpose:\r\n\r\n\r\nlangford <- function(x) {\r\n  ux = unique(x)\r\n  mux = match(x, ux)      \r\n  ans = integer(length(ux))       \r\n  for(i in seq_along(x)) ans[mux[i]] = i - ans[mux[i]]        \r\n  return(setNames(ans - 1L, ux))\r\n}\r\n\r\nlangford(y)\r\n\r\n E  F  A  D  C  B  G  H \r\n11  6  6 10  1  4  3  1 \r\n\r\n\r\nFinding a Langford Sequence\r\nSay we wanted to find a solution for 8 blocks. My strategy here is to keep sampling sequences until one satisfies the criteria. The way to check whether a sequence is a Langford sequence is to sort the intervals between each pair of letters and then ask if this is equal to 1:8. If it is, then we break and return that sequence.\r\n\r\n\r\nset.seed(17)\r\nwhile(TRUE) {\r\n  \r\n  samps <- sample(x)\r\n  tmp <- sort(langford(samps))\r\n  \r\n  if(sum(tmp==c(1:8))==8)\r\n    \r\n    break\r\n}\r\n\r\nsamps\r\n\r\n [1] \"G\" \"F\" \"D\" \"A\" \"E\" \"H\" \"A\" \"D\" \"F\" \"G\" \"E\" \"C\" \"B\" \"H\" \"B\" \"C\"\r\n\r\n\r\nWe can see that this is a Langford Sequence:\r\n\r\n\r\nsort(langford(samps))  ## the interval between pairs of letters in the sequence```\r\n\r\nB A C D E F H G \r\n1 2 3 4 5 6 7 8 \r\n\r\n\r\nThis is obviously a brute force approach, and not a cute algorithm that optimally finds solutions. We can generalize for any ‘n’:\r\n\r\n\r\n\r\nget.langford <- function(n){\r\n\r\n  \r\n  \r\nwhile(TRUE) {\r\n\r\n  x <- rep(LETTERS[1:n],2)\r\n  samps <- sample(x)\r\n  tmp <- sort(langford(samps))\r\n  \r\n  if(sum(tmp==c(1:n))==n)\r\n    \r\n    break\r\n}\r\n\r\nreturn(samps)\r\n}\r\n\r\n\r\nI could add some if-stop ‘s to this to prevent someone from entering a \\(n\\) that will not work (e.g. 5,6). Also, if someone wanted to use an \\(n\\) of >26 then we’d have to use combinations of letters. I don’t recommend this though, as the function is pretty slow for anything greater than \\(n=12\\).\r\nI think the strategy could also be optimized such that rather than randomly sampling sequences, the sequences tested are done so in a more systematic fashion.\r\nNevertheless, here it is working for \\(n=12\\).\r\n\r\n\r\nset.seed(77)\r\nlang12 <- get.langford(12)\r\nlang12\r\n\r\n [1] \"A\" \"L\" \"I\" \"L\" \"H\" \"B\" \"J\" \"G\" \"I\" \"C\" \"A\" \"E\" \"B\" \"H\" \"E\" \"D\"\r\n[17] \"F\" \"K\" \"G\" \"J\" \"F\" \"C\" \"K\" \"D\"\r\n\r\n\r\n\r\n\r\nsort(langford(lang12))\r\n\r\n L  E  F  K  I  B  D  H  A  G  C  J \r\n 1  2  3  4  5  6  7  8  9 10 11 12 \r\n\r\n\r\nWe can try and plot this with ggplot:\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\n# dataframe of position of each letter           \r\ndf <- data.frame(pos = 1:24, let = lang12)\r\n\r\n\r\n# get distances between each letter\r\nlang12dif <- sort(langford(lang12))\r\n\r\n# record start and end position of each letter\r\n# to make line between them\r\ndf1 <- df %>% group_by(let) %>% filter(pos==min(pos))%>%\r\n  full_join(data.frame(\r\n           let = names(lang12dif),\r\n           diff = lang12dif\r\n           )\r\n            ) %>%\r\n  mutate(end = diff+pos+1)\r\n           \r\n\r\n# reorder levels so can arrange in order\r\ndf$let <- factor(df$let, levels=names(lang12dif))\r\ndf1$let <- factor(df1$let, levels=names(lang12dif))\r\n\r\ndf %>%\r\n  ggplot() +\r\n  geom_segment(data=df1, aes(x=pos, xend=end, y=let, yend=let), color=\"gray44\")+\r\n  geom_tile(data=df, aes(x=pos, y=let, fill=let), color='black') +\r\n  geom_text(data=df, aes(x=pos, y=let, label=let)) +\r\n  theme_classic() +\r\n  geom_text(data=df1, aes(x=(pos+end)/2, y=let, label=diff),color='red', nudge_y = 0.25) +\r\n  theme(legend.position = 'none')\r\n\r\n\r\n\r\nFinding Skolem Sequences\r\nThis is simply just a change in the if statement prior to the break. Here we are looking for sequences that start with a gap of 0, and not 1.\r\nFor instance, a Skolem sequence for n=8:\r\n\r\n\r\nskolem <- function(n){\r\n\r\nwhile(TRUE) {\r\n\r\n  x <- rep(LETTERS[1:n],2)\r\n  samps <- sample(x)\r\n  tmp <- sort(langford(samps))\r\n  \r\n  if(sum(tmp==c(0:(n-1)))==n)\r\n    \r\n    break\r\n}\r\n\r\nreturn(samps)\r\n}\r\n\r\n\r\n\r\n\r\nset.seed(88)\r\nsk <- skolem(8)\r\nsk\r\n\r\n [1] \"B\" \"D\" \"B\" \"G\" \"E\" \"E\" \"F\" \"C\" \"G\" \"D\" \"A\" \"H\" \"F\" \"A\" \"C\" \"H\"\r\n\r\nsort(langford(sk))\r\n\r\nE B A H G F C D \r\n0 1 2 3 4 5 6 7 \r\n\r\n\r\nFinding Hooked Langford Sequences\r\nA Hooked Langford Sequence is one where another block/letter is inserted in the penultimate position in the sequence. They can be found for any \\(n\\):\r\nThe code for these Hooked Langford Sequence is here:\r\n\r\n\r\nhooked.langford <- function(n){\r\n\r\nwhile(TRUE) {\r\n\r\n  x <- rep(LETTERS[1:n],2)\r\n  samps <- sample(x)\r\n  samps <- c(head(samps,((n*2)-1)), \"X\", tail(samps, 1))\r\n\r\n  \r\n  tmp <- sort(langford(samps))\r\n  \r\n  if(sum(tmp==c(1:n))==n)\r\n    \r\n    break\r\n}\r\n\r\nreturn(samps)\r\n}\r\n\r\n Some examples:\r\n\r\n\r\nset.seed(1)\r\nh1 <- hooked.langford(5)\r\nh1\r\n\r\n [1] \"A\" \"D\" \"B\" \"E\" \"A\" \"E\" \"D\" \"C\" \"B\" \"X\" \"C\"\r\n\r\nsort(langford(h1))\r\n\r\nE C A D B X \r\n1 2 3 4 5 9 \r\n\r\n\r\n\r\n\r\nset.seed(11)\r\nh2 <- hooked.langford(9)\r\nh2\r\n\r\n [1] \"E\" \"H\" \"C\" \"H\" \"E\" \"D\" \"F\" \"G\" \"I\" \"A\" \"G\" \"C\" \"B\" \"D\" \"A\" \"I\"\r\n[17] \"F\" \"X\" \"B\"\r\n\r\nsort(langford(h2))\r\n\r\n H  G  E  A  B  I  D  C  F  X \r\n 1  2  3  4  5  6  7  8  9 17 \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-15-langford-sequences/langford-sequences_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-10-03T13:41:28-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-10-14-patterns-of-female-names-over-time/",
    "title": "Patterns of Female Names over Time",
    "description": "Investigating similarity of name popularity over time.",
    "author": [
      {
        "name": "James Curley",
        "url": "jamescurley.blog"
      }
    ],
    "date": "2020-10-15",
    "categories": [
      "babynames",
      "clustering",
      "time_series"
    ],
    "contents": "\r\n The babynames package put together by Hadley Wickham is a lot of fun for teaching R. This package contains a dataset of the same name that contains the number of boys and girls born each year since 1880 with every name.\r\nWhat you’ll notice if you play around with graphing the distribution of children born with particular names is that there are different patterns. Some names were popular long ago, some are only popular recently, others have had ups and downs in popularity. For a bit of fun, I thought it would be interesting to try and identify different patterns through principal components analysis (PCA) and clustering techniques.\r\nLet’s load the libraries we’ll need and the data:\r\n\r\n\r\n### load packages\r\nlibrary(babynames) \r\nlibrary(gridExtra)\r\nlibrary(tidyverse)\r\nlibrary(colorspace)\r\nlibrary(tsne)\r\n\r\n\r\nhead(babynames)\r\n\r\n# A tibble: 6 x 5\r\n   year sex   name          n   prop\r\n  <dbl> <chr> <chr>     <int>  <dbl>\r\n1  1880 F     Mary       7065 0.0724\r\n2  1880 F     Anna       2604 0.0267\r\n3  1880 F     Emma       2003 0.0205\r\n4  1880 F     Elizabeth  1939 0.0199\r\n5  1880 F     Minnie     1746 0.0179\r\n6  1880 F     Margaret   1578 0.0162\r\n\r\ntail(babynames)\r\n\r\n# A tibble: 6 x 5\r\n   year sex   name       n       prop\r\n  <dbl> <chr> <chr>  <int>      <dbl>\r\n1  2017 M     Zyhier     5 0.00000255\r\n2  2017 M     Zykai      5 0.00000255\r\n3  2017 M     Zykeem     5 0.00000255\r\n4  2017 M     Zylin      5 0.00000255\r\n5  2017 M     Zylis      5 0.00000255\r\n6  2017 M     Zyrie      5 0.00000255\r\n\r\n\r\nAs can be seen, this dataset records the number of children (n) of each name (name) that are boys or girls (sex) that were born in each year (year). The data go from 1880-2017. Additionally, the proportion (prop) of children born with that name in each year.\r\nTo plot the pattern of an individual name over time, I’m going to write a little custom function:\r\n\r\n\r\n# Function to plot a name over time\r\nplot_name <- function(bname = NULL) {\r\n  babynames %>%\r\n  filter(name==bname) %>% \r\n  ggplot(aes(year, n)) +\r\n  geom_line(aes(color=sex), lwd=1) +\r\n  scale_color_manual(values = c(\"firebrick1\", \"dodgerblue\")) +\r\n  theme_bw() +\r\n  ggtitle(bname)\r\n}\r\n\r\n\r\n\r\n\r\ngrid.arrange(\r\n  plot_name(\"Barbara\"),\r\n  plot_name(\"Megan\"),\r\n  plot_name(\"Jennifer\"),\r\n  plot_name(\"Irene\"),\r\n  ncol=2\r\n)\r\n\r\n\r\n\r\nWe’ll just focus on female names. These are the most popular female names of all time:\r\n\r\n\r\nbabynames %>%\r\n  filter(sex == \"F\") %>%\r\n  group_by(name) %>%\r\n  summarize(total = sum(n)) %>%\r\n  arrange(-total)\r\n\r\n# A tibble: 67,046 x 2\r\n   name        total\r\n   <chr>       <int>\r\n 1 Mary      4123200\r\n 2 Elizabeth 1629679\r\n 3 Patricia  1571692\r\n 4 Jennifer  1466281\r\n 5 Linda     1452249\r\n 6 Barbara   1434060\r\n 7 Margaret  1246649\r\n 8 Susan     1121440\r\n 9 Dorothy   1107096\r\n10 Sarah     1073895\r\n# ... with 67,036 more rows\r\n\r\n\r\nWe can also see that there are 67,046 unique female names in the dataset. Here are some of the least common names:\r\n\r\n\r\nbabynames %>%\r\n  filter(sex == \"F\") %>%\r\n  group_by(name) %>%\r\n  summarize(total = sum(n)) %>%\r\n  arrange(total)\r\n\r\n# A tibble: 67,046 x 2\r\n   name     total\r\n   <chr>    <int>\r\n 1 Aada         5\r\n 2 Aaden        5\r\n 3 Aadilynn     5\r\n 4 Aafreen      5\r\n 5 Aagot        5\r\n 6 Aaheli       5\r\n 7 Aaiyana      5\r\n 8 Aaja         5\r\n 9 Aakira       5\r\n10 Aakiyah      5\r\n# ... with 67,036 more rows\r\n\r\n\r\nIn fact there are 9,595 names that appear 5 times only in the dataset. To be entered into the data, each name must have been registered at least five times in a given year. So for all of these names, in one year there were five children born with that given name - and that was the only year.\r\n\r\n\r\nbabynames %>%\r\n  filter(sex == \"F\") %>%\r\n  group_by(name) %>%\r\n  summarize(total = sum(n)) %>%\r\n  filter(total==5) %>%\r\n  nrow()\r\n\r\n[1] 9595\r\n\r\n\r\nFor instance, the first name on the list - Aada - was given to five children in 2015:\r\n\r\n\r\nbabynames %>% filter(name == \"Aada\")\r\n\r\n# A tibble: 1 x 5\r\n   year sex   name      n       prop\r\n  <dbl> <chr> <chr> <int>      <dbl>\r\n1  2015 F     Aada      5 0.00000257\r\n\r\n\r\nReshape Data\r\nThe first exploration of these data that I’d like to do is to perform a PCA on the distribution of the frquency of names over years. This will give us a general idea of how many different ‘components/groups’ we might expect.\r\nTo do this, we need to have our data in a ‘wide’ format, with each column/variable representing a year and each row representing the total number of births that year for that particular name. We’ll put the names into rownames so we can keep that information, but only have numbers in the dataframe.\r\n\r\n\r\nbabywideF <- \r\n  babynames %>% \r\n  filter(sex==\"F\") %>% \r\n  select(name, year, n) %>%\r\n  pivot_wider(names_from = year, values_from = n, values_fill = list(n = 0))\r\n\r\nrownames(babywideF)<- babywideF %>% .$name  #set rownames\r\nbabywideF <- babywideF %>% select(-name) # remove name variable\r\n\r\n\r\nHere are the first 5 rows and 4 columns of the dataframe, as well as the rownames:\r\n\r\n\r\nbabywideF[1:5,1:4]\r\n\r\n# A tibble: 5 x 4\r\n  `1880` `1881` `1882` `1883`\r\n   <int>  <int>  <int>  <int>\r\n1   7065   6919   8148   8012\r\n2   2604   2698   3143   3306\r\n3   2003   2034   2303   2367\r\n4   1939   1852   2186   2255\r\n5   1746   1653   2004   2035\r\n\r\nrownames(babywideF)[1:5]\r\n\r\n[1] \"Mary\"      \"Anna\"      \"Emma\"      \"Elizabeth\" \"Minnie\"   \r\n\r\n\r\nPCA\r\nFor this sort of exploratory analysis, I’m going to simply use the default PCA function in R - princomp() and plot scree plots.\r\n\r\n\r\n### principal components analysis - females\r\nresF.pca <- princomp(babywideF)\r\nplot(resF.pca)\r\n\r\n\r\n\r\nThe above scree plot seems to indicate that there is one major component of names that accounts for the majority of variance. There then appears to be a few more components that account for a fair amount of variance. Depending on how micro-detailed we want to go, we could look for 4 or 5 components fairly reasonably. Though there may be something of interest in looking at 7 or 8 groups.\r\n\r\nClustering\r\nTo explore which names show more similar distribution patterns to one another over time, I’m going to firstly use k-means hierarchical clustering. There are pros and cons to all clustering methods. A problem with k-means is that you can get different clusters each time you run it due to how the method operates. An upside is that it is a fairly flexible method…\r\nThe clustering is done with the kmeans() function and by setting the number of clusters to find. After saving the results, we can look at how many individual names have been put into each cluster (the relative number of the cluster isn’t that important - e.g. if we re-run the clustering, the clusters with most names could be called by a different cluster number in future runs). I am going to set.seed() to make sure that this code is repeatable.\r\n\r\n\r\n###k-means clustering analysis\r\nset.seed(100)\r\nresF.k <- kmeans(babywideF, 6)\r\ntable(resF.k$cluster)\r\n\r\n    1     2     3     4     5     6 \r\n   14    19    71    15   119 66808 \r\n\r\n\r\nThe majority of names (66,808) are contained within the 1st component. The other five components have a much more manageable number of names.\r\nLet’s look at some of these names in more detail. We’ll start with the smallest cluster - remember, these are names that should show very similar trends over time in the frequency of births:\r\n\r\n\r\nnames(resF.k$cluster[resF.k$cluster==1])\r\n\r\n [1] \"Anna\"     \"Margaret\" \"Alice\"    \"Helen\"    \"Frances\"  \"Marie\"   \r\n [7] \"Ruth\"     \"Virginia\" \"Mildred\"  \"Evelyn\"   \"Betty\"    \"Dorothy\" \r\n[13] \"Doris\"    \"Shirley\" \r\n\r\n\r\nThis first group seems to make sense. You could well imagine someone called Mildred who had friends from her childhood called Dorothy and Betty.\r\nThe next smallest cluster has these names, which appear to be boomer-era names:\r\n\r\n\r\nnames(resF.k$cluster[resF.k$cluster==4])\r\n\r\n [1] \"Mary\"     \"Nancy\"    \"Susan\"    \"Barbara\"  \"Cynthia\"  \"Linda\"   \r\n [7] \"Kathleen\" \"Donna\"    \"Deborah\"  \"Carol\"    \"Karen\"    \"Patricia\"\r\n[13] \"Sharon\"   \"Sandra\"   \"Debra\"   \r\n\r\n\r\nThen we have this cluster, which look to be more children of the 1980s:\r\n\r\n\r\nnames(resF.k$cluster[resF.k$cluster==2])\r\n\r\n [1] \"Elizabeth\" \"Sarah\"     \"Laura\"     \"Amanda\"    \"Rebecca\"  \r\n [6] \"Amy\"       \"Christina\" \"Melissa\"   \"Angela\"    \"Jessica\"  \r\n[11] \"Lisa\"      \"Stephanie\" \"Kelly\"     \"Heather\"   \"Michelle\" \r\n[16] \"Jennifer\"  \"Ashley\"    \"Kimberly\"  \"Nicole\"   \r\n\r\n\r\nThe next biggest cluster are names that are more recent still:\r\n\r\n\r\nnames(resF.k$cluster[resF.k$cluster==3])\r\n\r\n [1] \"Emma\"      \"Ella\"      \"Grace\"     \"Julia\"     \"Katherine\"\r\n [6] \"Caroline\"  \"Katie\"     \"Hannah\"    \"Emily\"     \"Rachel\"   \r\n[11] \"Sara\"      \"Sophia\"    \"Victoria\"  \"Leah\"      \"Isabella\" \r\n[16] \"Olivia\"    \"Madeline\"  \"Samantha\"  \"Chloe\"     \"Molly\"    \r\n[21] \"Savannah\"  \"Ava\"       \"Abigail\"   \"Amber\"     \"Monica\"   \r\n[26] \"Natalie\"   \"Alicia\"    \"Courtney\"  \"Andrea\"    \"Crystal\"  \r\n[31] \"Jamie\"     \"Melanie\"   \"Sydney\"    \"Erin\"      \"Haley\"    \r\n[36] \"Gabrielle\" \"Alexandra\" \"Vanessa\"   \"Cassandra\" \"Jasmine\"  \r\n[41] \"Allison\"   \"Erica\"     \"Chelsea\"   \"Shannon\"   \"Shelby\"   \r\n[46] \"Paige\"     \"Jenna\"     \"April\"     \"Morgan\"    \"Megan\"    \r\n[51] \"Lauren\"    \"Lindsey\"   \"Alexis\"    \"Kayla\"     \"Mia\"      \r\n[56] \"Brooke\"    \"Danielle\"  \"Kristen\"   \"Tiffany\"   \"Kelsey\"   \r\n[61] \"Jordan\"    \"Alyssa\"    \"Taylor\"    \"Destiny\"   \"Brianna\"  \r\n[66] \"Brittany\"  \"Hailey\"    \"Katelyn\"   \"Madison\"   \"Kaitlyn\"  \r\n[71] \"Mackenzie\"\r\n\r\n\r\nFinally, we have a large cluster of 119 names. The relationship of these names to each other might take some more digging:\r\n\r\n\r\nnames(resF.k$cluster[resF.k$cluster==5])\r\n\r\n  [1] \"Annie\"      \"Clara\"      \"Florence\"   \"Martha\"     \"Carrie\"    \r\n  [6] \"Edith\"      \"Rose\"       \"Catherine\"  \"Lillian\"    \"Louise\"    \r\n [11] \"Ethel\"      \"Eva\"        \"Edna\"       \"Josephine\"  \"Ellen\"     \r\n [16] \"Charlotte\"  \"Jane\"       \"Irene\"      \"Kathryn\"    \"Esther\"    \r\n [21] \"Theresa\"    \"Pauline\"    \"Anne\"       \"Ann\"        \"Eleanor\"   \r\n [26] \"Maria\"      \"Ruby\"       \"Christine\"  \"Sylvia\"     \"Carolyn\"   \r\n [31] \"Sally\"      \"Sue\"        \"Jean\"       \"Jeanette\"   \"Lois\"      \r\n [36] \"Teresa\"     \"Loretta\"    \"Lucille\"    \"Regina\"     \"Roberta\"   \r\n [41] \"Norma\"      \"Annette\"    \"Janet\"      \"Juanita\"    \"Julie\"     \r\n [46] \"Vivian\"     \"Gladys\"     \"Rita\"       \"Tina\"       \"Anita\"     \r\n [51] \"Marjorie\"   \"Bonnie\"     \"June\"       \"Dolores\"    \"Peggy\"     \r\n [56] \"Connie\"     \"Jeanne\"     \"Joan\"       \"Wanda\"      \"Diana\"     \r\n [61] \"Marcia\"     \"Paula\"      \"Leslie\"     \"Geraldine\"  \"Debbie\"    \r\n [66] \"Phyllis\"    \"Suzanne\"    \"Elaine\"     \"Judith\"     \"Judy\"      \r\n [71] \"Lynn\"       \"Thelma\"     \"Audrey\"     \"Gloria\"     \"Gail\"      \r\n [76] \"Jo\"         \"Rosemary\"   \"Joyce\"      \"Dana\"       \"Eileen\"    \r\n [81] \"Lorraine\"   \"Denise\"     \"Laurie\"     \"Valerie\"    \"Tracy\"     \r\n [86] \"Beth\"       \"Yvonne\"     \"Cindy\"      \"Dawn\"       \"Joanne\"    \r\n [91] \"Renee\"      \"Carla\"      \"Jacqueline\" \"Joann\"      \"Beverly\"   \r\n [96] \"Janice\"     \"Pamela\"     \"Maureen\"    \"Darlene\"    \"Brenda\"    \r\n[101] \"Diane\"      \"Marilyn\"    \"Colleen\"    \"Robin\"      \"Jill\"      \r\n[106] \"Sheila\"     \"Vickie\"     \"Cheryl\"     \"Sherry\"     \"Kathy\"     \r\n[111] \"Rhonda\"     \"Vicki\"      \"Kim\"        \"Michele\"    \"Cathy\"     \r\n[116] \"Wendy\"      \"Terri\"      \"Lori\"       \"Tammy\"     \r\n\r\n\r\nJust for completeness, here is a random sample of 10 names from ‘cluster 6’. As can be seen, these tend to be uncommon names.\r\n\r\n\r\nset.seed(17)\r\nsample(names(resF.k$cluster[resF.k$cluster==6]),10)\r\n\r\n [1] \"Ronna\"      \"Yaely\"      \"Sarahmarie\" \"Kiyasha\"    \"Norell\"    \r\n [6] \"Jaleeza\"    \"Darlina\"    \"Chari\"      \"Kanaiya\"    \"Sherridan\" \r\n\r\n\r\nRepeat the Process?\r\nIt might be more beneficial to repeat this process, but exclude the less common names. To do this we will filter our data by not keeping any names that appear in cluster 6. This leaves us with 238 names.\r\n\r\n\r\ngroup1x <- names(resF.k$cluster[resF.k$cluster<6])\r\nlength(group1x)\r\n\r\n[1] 238\r\n\r\n\r\nWe can only keep these names, and redo our PCA:\r\n\r\n\r\nbabywideF1 <- babywideF %>%  filter(rownames(.) %in% group1x) \r\n\r\n### principal components analysis - females\r\nresF1.pca <- princomp(babywideF1)\r\nplot(resF1.pca)\r\n\r\n\r\n\r\nThe scree plot again indicates approximately three or four main components, plus perhaps 3 or 4 ‘fringe’ ones. Let’s redo our K-means clustering with 7 clusters, just because it might be more fun/interesting to try and split names up as much as possible to see if it makes logical sense.\r\n\r\n\r\n###k-means clustering analysis\r\nset.seed(10)\r\nresF1.k <- kmeans(babywideF1, 7)\r\ntable(resF1.k$cluster)\r\n\r\n 1  2  3  4  5  6  7 \r\n73  5 16 18 67  4 55 \r\n\r\n\r\nAgain, let’s look at these in a bit more detail, this time looking from smallest group to largest. The first four appear to be older names. When we can compare their distributions on a plot, we can see that they are very similar having large peaks in the 1950s - although Mary also has a large peak in the 1920s:\r\n\r\n\r\nbabynames %>%\r\n  filter(sex==\"F\") %>%\r\n  filter(name %in% group1x[resF1.k$cluster==6]) %>% \r\n  ggplot(aes(year, n)) +\r\n  geom_line(aes(color=name, group=name), lwd=1) +\r\n  theme_classic() +\r\n  scale_color_discrete_qualitative(palette = \"Dark 2\")\r\n\r\n\r\n\r\nThe next cluster of five names are even older names, all peaking in the 1920s:\r\n\r\n\r\nbabynames %>%\r\n  filter(sex==\"F\") %>%\r\n  filter(name %in% group1x[resF1.k$cluster==2]) %>% \r\n  ggplot(aes(year, n)) +\r\n  geom_line(aes(color=name, group=name), lwd=1) +\r\n  theme_classic() +\r\n  scale_color_discrete_qualitative(palette = \"Dark 2\")\r\n\r\n\r\n\r\nIf I’m being picky, I would suggest that the peak of Betty is just after the other four names, and Margaret has a second peak in the 1950s. It’s possible we could find other names that match these patterns, but grouping these five together obviously has some merits.\r\nThe next group is cluster 3, which has 16 names - all very popular names during the 1950s & 1960s:\r\n\r\n\r\nbabynames %>%\r\n  filter(sex==\"F\") %>%\r\n  filter(name %in% group1x[resF1.k$cluster==3]) %>% \r\n  ggplot(aes(year, n)) +\r\n  geom_line(aes(color=name, group=name), alpha = .4) +\r\n  theme_classic() +\r\n  scale_color_discrete_qualitative(palette = \"Harmonic\")\r\n\r\n\r\n\r\nThe next group is cluster 4, which has 18 names:\r\n\r\n\r\nbabynames %>%\r\n  filter(sex==\"F\") %>%\r\n  filter(name %in% group1x[resF1.k$cluster==4]) %>% \r\n  ggplot(aes(year, n)) +\r\n  geom_line(aes(color=name, group=name), alpha=.4) +\r\n  theme_classic() \r\n\r\n\r\n\r\nThis group looks to be generally names that were popular during the 1970s and 1980s. However, there are a couple of names in here which look a bit out of place. The green line that has a large peak in the 1920s is Elizabeth. The blue line with a peak just after the 1960s is Lisa. The green line with the largest peak in the early 1970s is Jennifer. This name does have a similar pattern to the others - it’s just that it is so popular it has an elevated peak.\r\nHere are Elizabeth and Lisa separated from the rest - Elizabeth is trimodal in its distribution having multiple periods of poularity!\r\n\r\n\r\ngrid.arrange(\r\n  plot_name(\"Elizabeth\"),\r\n  plot_name(\"Lisa\"),\r\n  ncol=2\r\n)\r\n\r\n\r\n\r\nThe last three clusters - clusters 1, 5 and 7 have between 55-73 names each. Here, I’ll just plot each without showing the names.\r\nCluster 5 appears to include names from the 1990s, some having mini-peaks in the 1920s:\r\n\r\n\r\nbabynames %>%\r\n  filter(sex==\"F\") %>%\r\n  filter(name %in% group1x[resF1.k$cluster==5]) %>% \r\n  ggplot(aes(year, n)) +\r\n  geom_line(aes(group=name), color=\"#123abc\", lwd=1, alpha=.2) +\r\n  theme_classic() +\r\n  theme(legend.position = 'none')\r\n\r\n\r\n\r\nThree of the names that appear to have had the peaks in the 1920s as well as post 1990s are Ella, Grace and Julia:\r\n\r\n\r\nbabynames %>%\r\n  filter(sex==\"F\") %>%\r\n  filter(name==\"Grace\" | name==\"Julia\" | name==\"Ella\") %>% \r\n  ggplot(aes(year, n)) +\r\n  geom_line(aes(color=name), lwd=1) +\r\n  theme_classic()\r\n\r\n\r\n\r\nCluster 7 appears to include names from the 1960s, being popular a litle after the boomers of cluster 3.\r\n\r\n\r\nbabynames %>%\r\n  filter(sex==\"F\") %>%\r\n  filter(name %in% group1x[resF1.k$cluster==7]) %>% \r\n  ggplot(aes(year, n)) +\r\n  geom_line(aes(group=name), color=\"lightseagreen\", lwd=1, alpha=.2) +\r\n  theme_classic() +\r\n  theme(legend.position = 'none')\r\n\r\n\r\n\r\nCluster 1 is a little more mixed. There are clearly some 1920s names in here, but also possibly some other pre WW2 popular names.\r\n\r\n\r\nbabynames %>%\r\n  filter(sex==\"F\") %>%\r\n  filter(name %in% group1x[resF1.k$cluster==1]) %>% \r\n  ggplot(aes(year, n)) +\r\n  geom_line(aes(group=name), color=\"orange\", lwd=1, alpha=.2) +\r\n  theme_classic() +\r\n  theme(legend.position = 'none')\r\n\r\n\r\n\r\nTSNE mapping\r\nAn intriguing way of mapping multidimensional data into a 2d plot is to use T-distributed Stochastic Neighbor Embedding. This can be done in the R package tsne.\r\nBelow I create a distance object, D, on which we will run the tsne analysis. I also create a dataframe to store the names, cluster id, and final coordinates of each name.\r\n\r\n\r\nD <- dist(babywideF1)  #create distance object\r\n\r\n\r\n# creating dataframe for plotting colors and text on final plot\r\n\r\ncluster1 <- group1x[resF1.k$cluster==1]\r\ncluster2 <- group1x[resF1.k$cluster==2]\r\ncluster3 <- group1x[resF1.k$cluster==3]\r\ncluster4 <- group1x[resF1.k$cluster==4]\r\ncluster5 <- group1x[resF1.k$cluster==5]\r\ncluster6 <- group1x[resF1.k$cluster==6]\r\ncluster7 <- group1x[resF1.k$cluster==7]\r\n\r\n\r\n\r\nnamesdf <- data.frame(\r\n              name = c(cluster1, cluster2, cluster3, cluster4, \r\n                       cluster5, cluster6, cluster7), \r\n              group = c(rep(1, length(cluster1)), \r\n                        rep(2, length(cluster2)), \r\n                        rep(3, length(cluster3)), \r\n                        rep(4, length(cluster4)), \r\n                        rep(5, length(cluster5)), \r\n                        rep(6, length(cluster6)), \r\n                        rep(7, length(cluster7)))\r\n           )\r\n                    \r\n\r\nnamesdf <- namesdf[match(group1x, namesdf$name),] #names in correct order to match rownames of babywideF1 \r\n\r\ncolors = rainbow(7)\r\nnames(colors) = unique(namesdf$group)\r\n\r\n\r\n#define function used in plotting\r\necb = function(x,y){ plot(x,t='n'); text(x,labels=rownames(babywideF1), col=colors[namesdf$group], cex=1) }\r\n\r\n\r\nset.seed(100)\r\ntsne_D <- tsne(D, k=2,  epoch_callback = ecb, perplexity=50)\r\n\r\n\r\nWe can add the output of tsne into the dataframe:\r\n\r\n\r\n\r\nnamesdf$x <- tsne_D[,1]\r\nnamesdf$y <- tsne_D[,2]             \r\n\r\nhead(namesdf)\r\n\r\n         name group           x          y\r\n180      Mary     6 -13.2455975  -4.889826\r\n1        Anna     1   0.1428635 -11.981969\r\n113      Emma     5  12.1987696  -1.243631\r\n95  Elizabeth     4  -1.7484964   9.684795\r\n74   Margaret     2  -4.7970545 -11.556546\r\n2       Alice     1  -1.9868251 -10.067491\r\n\r\n\r\nRather than using the default plot of the tsne package, I have plotted the data using ggplot2 in 2D space:\r\n\r\n\r\n\r\nggplot(namesdf, aes(x=x, y=y, color=factor(group), label=name)) +\r\n  geom_text(size=2) +\r\n  theme_classic() +\r\n  theme(legend.position='none')\r\n\r\n\r\n\r\n\r\nThis is pretty cool. This data reduction and visualization method actually maps pretty well to what we did before. The different groups are denoted by different colors. Some logical patterns emerge.\r\nThe 1950s names - Barbara, Linda, Mary and Patricia are together in purple on the left hand side. Close by in lime green are the boomer names that were popular a little later. At the bottom in yellow are the names that were really popular in the 1920s (e.g. Dorothy, Helen).\r\nThe names that have become very popular recently (e.g. Isabella, Madison, Abigail) are on the far right side of visual space in light blue. Interestingly, Grace and Julia position next to each other with Ella not too far away - these are the names that were popular early and then have had a recent resurgence.\r\nThe names in green at the topare those such as Jennifer that were very popular in the late 70s and 80s. Significantly, Lisa and Kimberly are close to each other - this might be because their peaks were pre 1975. Julie is another with pre-1975 peaks, but that name is in a different cluster. The names in blue close to the green group are the names that blossomed in the 80s and 90s before more recently declining in popularity such as Megan and Lauren.\r\nFinally, much could be done to look for further patterns in the red and pink groups, but that will have to wait for another time ! There is also much that can be done with the less common names - again, we’ll look at those another time.\r\n\r\n\r\n",
    "preview": "posts/2020-10-14-patterns-of-female-names-over-time/patterns-of-female-names-over-time_files/figure-html5/unnamed-chunk-32-1.png",
    "last_modified": "2021-10-03T13:41:28-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-10-13-world-cup-birthday-problem/",
    "title": "World Cup Birthday Problem",
    "description": "Do world cup squad members share birthdays?",
    "author": [
      {
        "name": "James Curley",
        "url": "jamescurley.blog"
      }
    ],
    "date": "2020-10-14",
    "categories": [
      "puzzles",
      "regex",
      "soccer"
    ],
    "contents": "\r\nThe Birthday Problem\r\nThe Birthday Problem is quite a well known puzzle. If you attended a party of 20 people or so and were asked how likely it was that any two people at the party shared a birthday - what would you think? Most people when they are first confronted with this question think the probability will be quite small. After all, there are 365 days in the year, and only 20 people at the party. However, it turns out that the chances are pretty decent.\r\nThere is, obviously, some probability mathematics behind this that will tell us that the chances of any two people sharing a birthday at the party of 20 people is around 41%. You can read more about the math here.\r\nInstead of going through the math, I’d like to show this visually using a simulation. Then we’ll check this problem against some real world data - using soccer world cup squads.\r\n\r\nSimulating the Birthday Problem\r\nWhat we’re doing below is to use sample() to randomly select n number of numbers between 1 and 365. These are our days of the year - we’re just going to ignore leap years. Importantly, we’re selecting these numbers WITH replacement.\r\nFor example, let’s select five numbers between 1 and 365 with replacement\r\n\r\n\r\n\r\n\r\n\r\nsample(1:365, 5, T)\r\n\r\n[1]  97 136 210 332  74\r\n\r\n\r\nAs you can see from these numbers, none are duplicated. However, if we got 25 numbers, then we have more chance of getting some numbers twice:\r\n\r\n\r\nvs <- sample(1:365, 25, T)\r\nvs\r\n\r\n [1] 328 345 242 230  23  76  65 251 141 281 182 262 363 139 284 342\r\n[17]  78 238  46  98 141   5 140 318 125\r\n\r\n\r\nWe next use the function duplicated() to find out if any of the numbers are …. duplicated.\r\n\r\n\r\nduplicated(vs)\r\n\r\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\r\n[23] FALSE FALSE FALSE\r\n\r\nHere, the 21st number is duplicated - which is 141. It appears twice in the list - this would correspond to a shared birthday of May 20th (the 141st day of the year).\r\n\r\nThe code below runs 20,000 lots of group sizes of 1-50. For each group size, and for each permutation, we count how many of the birthdays are duplicated:\r\n\r\n\r\n\r\n\r\n\r\n\r\nnperm <- 20000\r\nmaxn <- 50\r\n\r\nres <- matrix(NA, nrow = maxn, ncol = nperm)\r\n\r\nfor(n in 1:maxn) {\r\n  for(i in 1:nperm) {\r\n    x <- sample(1:365, n, T)\r\n    res[[n,i]] <- sum(duplicated(x))\r\n  }\r\n}\r\n\r\n\r\nThe table below visualizes the first 5 iterations of group sizes of 1-15. Obviously, if it’s a party of one person it’s a lame party and also nobody is going to share a birthday. What you can see in the results below, is that in the 3rd run, people shared birthdays when there were 8 or 15 people at the party. In the 4th run, people shared birthdays when there were 12 or 14 people at the party.\r\n\r\n\r\n\r\n# first 5 iterations of group sizes 1-15\r\nres[1:15,1:5]\r\n\r\n      [,1] [,2] [,3] [,4] [,5]\r\n [1,]    0    0    0    0    0\r\n [2,]    0    0    0    0    0\r\n [3,]    0    0    0    0    0\r\n [4,]    0    0    0    0    0\r\n [5,]    0    0    0    0    0\r\n [6,]    0    0    0    0    0\r\n [7,]    0    0    0    0    0\r\n [8,]    0    0    1    0    0\r\n [9,]    0    0    0    0    0\r\n[10,]    0    0    0    0    0\r\n[11,]    0    0    0    0    0\r\n[12,]    0    1    0    1    0\r\n[13,]    0    0    0    0    0\r\n[14,]    0    0    0    1    0\r\n[15,]    1    0    1    0    0\r\n\r\n\r\nFor each group size, we can count over all 20,000 iterations how many of the 20,000 instances had at least two people that shared a birthday. We can then work out the proportion of times this happens:\r\n\r\n\r\n\r\nv <- apply(res, 1, function(x) sum(x>0))\r\nround(v/nperm,2)\r\n\r\n [1] 0.00 0.00 0.01 0.02 0.03 0.04 0.06 0.08 0.09 0.12 0.14 0.16 0.19\r\n[14] 0.22 0.25 0.28 0.31 0.34 0.38 0.41 0.44 0.48 0.51 0.54 0.57 0.60\r\n[27] 0.62 0.65 0.68 0.71 0.73 0.75 0.77 0.80 0.81 0.84 0.85 0.86 0.88\r\n[40] 0.89 0.90 0.91 0.93 0.93 0.94 0.95 0.96 0.96 0.97 0.97\r\n\r\n\r\nThis shows you that when there are only 5 people at the party, you have a 3% chance of there being shared birthdays. When there are 40 people at a party, there is a 89% chance of there being a shared birthday. It turns out that when there are 23 people at a party there is a 51% chance of there being a shared birthday. 23 is the number of people where it’s more likely for there to be people that share a birthday than not.\r\nWe can make a simple visualization of this. The solid black line represents the probability (y-axis) of at least two people sharing a birthday for a given group size (x-axis):\r\n\r\n\r\n\r\nplot(1:maxn, v/nperm, type='l', lwd=2,\r\n     xlab=\"Party Size\", ylab=\"Probability of Birthday Being Shared\")\r\nabline(h=0.5, col=\"red\",lty=2)\r\nabline(v=23, col=\"black\",lty=3)\r\n\r\n\r\n\r\nWorld Cup Data\r\nWhen soccer teams compete at major championships, coaches have to pick squads of 23 for the tournament. Obviously, based on the above, this is a fairly interesting number. It led me to wonder, what proportion of world cup squads have players that shared birthdays?\r\nThe 32 squads for the 2018 world cup are available here. What I’m doing below is to use the R package rvest to scrape these tables from wikipedia.\r\n\r\n\r\n\r\nlibrary(rvest)    \r\nURL <- \"https://en.wikipedia.org/wiki/2018_FIFA_World_Cup_squads\"\r\n\r\nsquads <- URL %>% \r\n  read_html %>%\r\n  html_nodes(\"table\") %>%\r\n  html_table() %>%\r\n  head(32)\r\n\r\n\r\nThe squads are stored in a list. Let’s look at the first four rows of the seventh squad, which is Portugal. It’s a bit ugly to look at because it’s split over several rows, but hopefully you can see what type of data we have:\r\n\r\n\r\nsquads[[7]][1:4,]\r\n\r\n  No. Pos.           Player                    Date of birth (age)\r\n1   1  1GK     Rui Patrício (1988-02-15)15 February 1988 (aged 30)\r\n2   2  2DF      Bruno Alves (1981-11-27)27 November 1981 (aged 36)\r\n3   3  2DF             Pepe (1983-02-26)26 February 1983 (aged 35)\r\n4   4  3MF Manuel Fernandes  (1986-02-05)5 February 1986 (aged 32)\r\n  Caps Goals             Club\r\n1   69     0      Sporting CP\r\n2   96    11          Rangers\r\n3   95     5         Besiktas\r\n4   14     3 Lokomotiv Moscow\r\n\r\n\r\nEach table has the following column names:\r\n\r\n\r\ncolnames(squads[[1]])\r\n\r\n[1] \"No.\"                 \"Pos.\"                \"Player\"             \r\n[4] \"Date of birth (age)\" \"Caps\"                \"Goals\"              \r\n[7] \"Club\"               \r\n\r\n\r\nAs you might notice, the date of birth column is a bit of a mess, which we’ll deal with shortly. There is also no ‘country’ column, which would be useful. To create this, I realized that on the same wikipedia page, if we grabbed all text that was written with a <h3> HTML tag, we could get the country names and add them in to each squad - after getting rid of a bit of extraneous text that comes along at the end of each them when we scrape the data.\r\nShown below are the first 4 rows of the 23rd squad, South Korea, once we’ve added in the country name:\r\n\r\n\r\n\r\nnames(squads) <- URL %>% \r\n  read_html %>%\r\n  html_nodes(\"h3\") %>%\r\n  html_text() %>%\r\n  head(32) %>%\r\n  gsub(\"\\\\[edit\\\\]\",\"\",.)\r\n\r\nsquads <- Map(cbind, squads, country = names(squads))\r\n\r\nsquads[[23]][1:4,]\r\n\r\n  No. Pos.          Player                     Date of birth (age)\r\n1   1  1GK   Kim Seung-gyu (1990-09-30)30 September 1990 (aged 27)\r\n2   2  2DF        Lee Yong  (1986-12-24)24 December 1986 (aged 31)\r\n3   3  2DF Jung Seung-hyun      (1994-04-03)3 April 1994 (aged 24)\r\n4   4  2DF      Oh Ban-suk       (1988-05-20)20 May 1988 (aged 30)\r\n  Caps Goals                   Club     country\r\n1   33     0            Vissel Kobe South Korea\r\n2   28     0 Jeonbuk Hyundai Motors South Korea\r\n3    6     0             Sagan Tosu South Korea\r\n4    2     0            Jeju United South Korea\r\n\r\n\r\nWhat we really care about for this post is whether players in the same squad share a birthday. Let’s illustrate this looking at South Korea’s birthdays.\r\nHere are all the birthdays as listed in the date of birth column:\r\n\r\n\r\nx <- squads[[23]]$`Date of birth (age)`\r\nx\r\n\r\n [1] \"(1990-09-30)30 September 1990 (aged 27)\"\r\n [2] \"(1986-12-24)24 December 1986 (aged 31)\" \r\n [3] \"(1994-04-03)3 April 1994 (aged 24)\"     \r\n [4] \"(1988-05-20)20 May 1988 (aged 30)\"      \r\n [5] \"(1988-10-04)4 October 1988 (aged 29)\"   \r\n [6] \"(1987-01-16)16 January 1987 (aged 31)\"  \r\n [7] \"(1992-07-08)8 July 1992 (aged 25)\"      \r\n [8] \"(1990-10-30)30 October 1990 (aged 27)\"  \r\n [9] \"(1988-04-14)14 April 1988 (aged 30)\"    \r\n[10] \"(1998-01-06)6 January 1998 (aged 20)\"   \r\n[11] \"(1996-01-26)26 January 1996 (aged 22)\"  \r\n[12] \"(1990-02-25)25 February 1990 (aged 28)\" \r\n[13] \"(1989-02-27)27 February 1989 (aged 29)\" \r\n[14] \"(1990-09-17)17 September 1990 (aged 27)\"\r\n[15] \"(1989-12-14)14 December 1989 (aged 28)\" \r\n[16] \"(1989-01-24)24 January 1989 (aged 29)\"  \r\n[17] \"(1992-08-10)10 August 1992 (aged 25)\"   \r\n[18] \"(1992-06-09)9 June 1992 (aged 26)\"      \r\n[19] \"(1990-02-27)27 February 1990 (aged 28)\" \r\n[20] \"(1991-09-28)28 September 1991 (aged 26)\"\r\n[21] \"(1987-07-06)6 July 1987 (aged 30)\"      \r\n[22] \"(1988-03-10)10 March 1988 (aged 30)\"    \r\n[23] \"(1991-09-25)25 September 1991 (aged 26)\"\r\n\r\n\r\nWe don’t need all this stuff, so we can grab everything inside the brackets using a regex. To be honest, I’m sure there is an easier way of doing this - this is just the way I could figure out quickly:\r\n\r\n\r\n#get all inside brackets\r\ngsub(\"\\\\(([^()]*)\\\\)|.\", \"\\\\1\", x, perl=T)\r\n\r\n [1] \"1990-09-30aged 27\" \"1986-12-24aged 31\" \"1994-04-03aged 24\"\r\n [4] \"1988-05-20aged 30\" \"1988-10-04aged 29\" \"1987-01-16aged 31\"\r\n [7] \"1992-07-08aged 25\" \"1990-10-30aged 27\" \"1988-04-14aged 30\"\r\n[10] \"1998-01-06aged 20\" \"1996-01-26aged 22\" \"1990-02-25aged 28\"\r\n[13] \"1989-02-27aged 29\" \"1990-09-17aged 27\" \"1989-12-14aged 28\"\r\n[16] \"1989-01-24aged 29\" \"1992-08-10aged 25\" \"1992-06-09aged 26\"\r\n[19] \"1990-02-27aged 28\" \"1991-09-28aged 26\" \"1987-07-06aged 30\"\r\n[22] \"1988-03-10aged 30\" \"1991-09-25aged 26\"\r\n\r\n\r\nClearly, this is still too much info. However, because the data is so uniform in type, we can use substr() to get the text from the 6th to 10th character of each string. That corresponds to the month and day of birth (years aren’t important for this problem):\r\n\r\n\r\n#day-month\r\nvec <- substr(gsub(\"\\\\(([^()]*)\\\\)|.\", \"\\\\1\", x, perl=T),6,10)\r\nvec\r\n\r\n [1] \"09-30\" \"12-24\" \"04-03\" \"05-20\" \"10-04\" \"01-16\" \"07-08\" \"10-30\"\r\n [9] \"04-14\" \"01-06\" \"01-26\" \"02-25\" \"02-27\" \"09-17\" \"12-14\" \"01-24\"\r\n[17] \"08-10\" \"06-09\" \"02-27\" \"09-28\" \"07-06\" \"03-10\" \"09-25\"\r\n\r\n\r\nWe can used duplicated() to now return all the dates that are duplicates:\r\n\r\n\r\nduplicated(vec) | duplicated(vec, fromLast=TRUE)\r\n\r\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[12] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\r\n[23] FALSE\r\n\r\nvec[duplicated(vec) | duplicated(vec, fromLast=TRUE)]\r\n\r\n[1] \"02-27\" \"02-27\"\r\n\r\n\r\nSo, in the South Korean squad, there are two players who share birthdays on 27th February. These players happen to be Koo Ja-cheol & Kim Young-gwon - but this is all revealed at the bottom.\r\n\r\nThe below is a custom function using the above logic to check each squad for duplicated birthdays and to put them into each squad’s dataframe:\r\n\r\n\r\ndob_dup <- function(df){\r\n  x <- df$`Date of birth (age)`\r\n  vec <- substr(gsub(\"\\\\(([^()]*)\\\\)|.\", \"\\\\1\", x, perl=T),6,10)\r\n  df$daymonth <- vec\r\n  df$dupl <- duplicated(vec) | duplicated(vec, fromLast=TRUE)\r\n  return(df)\r\n}\r\n\r\n\r\nHere, we apply the function to all squads (using map() from tidyverse to be able to apply the function over each squad dataframe in the list). I’m showing you Morocco’s first five players as an example:\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\nsquads <- squads %>% map(dob_dup)\r\nsquads[[6]][1:5,]\r\n\r\n  No. Pos.                  Player\r\n1   1  1GK          Yassine Bounou\r\n2   2  2DF           Achraf Hakimi\r\n3   3  2DF            Hamza Mendyl\r\n4   4  2DF         Manuel da Costa\r\n5   5  2DF Medhi Benatia (captain)\r\n                    Date of birth (age) Caps Goals\r\n1    (1991-04-05)5 April 1991 (aged 27)   11     0\r\n2 (1998-11-04)4 November 1998 (aged 19)   10     1\r\n3 (1997-10-21)21 October 1997 (aged 20)   13     0\r\n4      (1986-05-06)6 May 1986 (aged 32)   28     1\r\n5   (1987-04-17)17 April 1987 (aged 31)   57     2\r\n                 Club country daymonth  dupl\r\n1              Girona Morocco    04-05  TRUE\r\n2         Real Madrid Morocco    11-04 FALSE\r\n3               Lille Morocco    10-21 FALSE\r\n4 Istanbul Basaksehir Morocco    05-06 FALSE\r\n5            Juventus Morocco    04-17 FALSE\r\n\r\n\r\nApplying this to every squad, we can then count up the number of players that share a birthday in each squad of 23 players:\r\n\r\n\r\nv <- squads %>% map(~ sum(.$dupl)) %>% unlist()\r\nv\r\n\r\n       Egypt       Russia Saudi Arabia      Uruguay         Iran \r\n           0            2            0            0            2 \r\n     Morocco     Portugal        Spain    Australia      Denmark \r\n           4            6            2            2            0 \r\n      France         Peru    Argentina      Croatia      Iceland \r\n           2            2            0            2            0 \r\n     Nigeria       Brazil   Costa Rica       Serbia  Switzerland \r\n           2            4            2            0            0 \r\n     Germany       Mexico  South Korea       Sweden      Belgium \r\n           2            0            2            0            0 \r\n     England       Panama      Tunisia     Colombia        Japan \r\n           2            0            0            0            0 \r\n      Poland      Senegal \r\n           8            0 \r\n\r\n\r\nPoland lead the way with 8 players sharing birthdays ! Here they are:\r\n\r\n\r\nsquads[[31]] %>% filter(dupl==T) %>% select(1:3,5,7,9)\r\n\r\n  No. Pos.              Player Caps                 Club daymonth\r\n1   1  1GK   Wojciech Szczesny   35             Juventus    04-18\r\n2   2  2DF       Michal Pazdan   33         Legia Warsaw    09-21\r\n3   6  3MF      Jacek Góralski    5   Ludogorets Razgrad    09-21\r\n4  10  3MF Grzegorz Krychowiak   51 West Bromwich Albion    01-29\r\n5  14  4FW   Lukasz Teodorczyk   17           Anderlecht    06-03\r\n6  20  2DF     Lukasz Piszczek   63    Borussia Dortmund    06-03\r\n7  21  3MF       Rafal Kurzawa    3        Górnik Zabrze    01-29\r\n8  22  1GK    Lukasz Fabianski   45         Swansea City    04-18\r\n\r\n\r\nSo, how many of the thirty-two 23 man squads had at least two players that shared a birthday ?\r\n\r\n\r\nv[v>0]\r\n\r\n     Russia        Iran     Morocco    Portugal       Spain \r\n          2           2           4           6           2 \r\n  Australia      France        Peru     Croatia     Nigeria \r\n          2           2           2           2           2 \r\n     Brazil  Costa Rica     Germany South Korea     England \r\n          4           2           2           2           2 \r\n     Poland \r\n          8 \r\n\r\n\r\nIt turns out that 16/32 squads had players that shared birthdays - so 50% which is basically what our simulation above told us was the probability.\r\nHowever, this is not probably the full story. You may notice that we have more shared birthdays than two in many squads. Interestingly, it turns out that soccer squads may not be completely random samples with respect to birthdays. Often in sports teams, soccer included, players tend to be born in certain months of the year. This is often due to older children in age groups being picked for school teams and so on. We may well have some sampling bias in our data - something we can explore in another post.\r\n\r\nOut of interest, here is the full list of all players that share birthdays in squads:\r\n\r\n\r\nsquads %>% map(dob_dup) %>% map(~filter(., dupl==T)) %>%\r\n  map(~ select(., c(3, 5,8:9))) %>% data.table::rbindlist()\r\n\r\n                         Player Caps     country daymonth\r\n 1:           Aleksei Miranchuk   18      Russia    10-17\r\n 2:             Anton Miranchuk    6      Russia    10-17\r\n 3:               Saman Ghoddos    8        Iran    09-06\r\n 4:            Pejman Montazeri   46        Iran    09-06\r\n 5:              Yassine Bounou   11     Morocco    04-05\r\n 6:             Younès Belhanda   47     Morocco    02-25\r\n 7:                 Nabil Dirar   34     Morocco    02-25\r\n 8:        Ahmed Reda Tagnaouti    2     Morocco    04-05\r\n 9:            Manuel Fernandes   14    Portugal    02-05\r\n10:           Raphaël Guerreiro   24    Portugal    12-22\r\n11:                  José Fonte   31    Portugal    12-22\r\n12: Cristiano Ronaldo (captain)  150    Portugal    02-05\r\n13:               João Moutinho  110    Portugal    09-08\r\n14:             Bruno Fernandes    6    Portugal    09-08\r\n15:                        Koke   40       Spain    01-08\r\n16:                 David Silva  121       Spain    01-08\r\n17:                 Aziz Behich   23   Australia    12-16\r\n18:                   Tom Rogic   37   Australia    12-16\r\n19:             Benjamin Pavard    6      France    03-28\r\n20:              Steve Mandanda   27      France    03-28\r\n21:            Jefferson Farfán   84        Peru    10-26\r\n22:               Nilson Loyola    3        Peru    10-26\r\n23:               Mateo Kovacic   41     Croatia    05-06\r\n24:                 Marko Pjaca   16     Croatia    05-06\r\n25:               Wilfred Ndidi   17     Nigeria    12-16\r\n26:              Tyronne Ebuehi    7     Nigeria    12-16\r\n27:                     Alisson   26      Brazil    10-02\r\n28:                 Filipe Luís   33      Brazil    08-09\r\n29:                     Willian   57      Brazil    08-09\r\n30:             Roberto Firmino   21      Brazil    10-02\r\n31:                Bryan Oviedo   44  Costa Rica    02-18\r\n32:                David Guzmán   43  Costa Rica    02-18\r\n33:                 Niklas Süle   11     Germany    09-03\r\n34:              Jérôme Boateng   71     Germany    09-03\r\n35:                Koo Ja-cheol   68 South Korea    02-27\r\n36:              Kim Young-gwon   53 South Korea    02-27\r\n37:                 Kyle Walker   35     England    05-28\r\n38:                 John Stones   26     England    05-28\r\n39:           Wojciech Szczesny   35      Poland    04-18\r\n40:               Michal Pazdan   33      Poland    09-21\r\n41:              Jacek Góralski    5      Poland    09-21\r\n42:         Grzegorz Krychowiak   51      Poland    01-29\r\n43:           Lukasz Teodorczyk   17      Poland    06-03\r\n44:             Lukasz Piszczek   63      Poland    06-03\r\n45:               Rafal Kurzawa    3      Poland    01-29\r\n46:            Lukasz Fabianski   45      Poland    04-18\r\n                         Player Caps     country daymonth\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-13-world-cup-birthday-problem/world-cup-birthday-problem_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-10-03T13:41:28-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-10-13-cat-names/",
    "title": "Cat Names",
    "description": "Identifying cats that are named after animals.",
    "author": [
      {
        "name": "James Curley",
        "url": "jamescurley.blog"
      }
    ],
    "date": "2020-10-13",
    "categories": [
      "cats",
      "regex"
    ],
    "contents": "\r\nOne of my favorite datasets for teaching is the Austin Animal Center Intake Dataset. The Austin Animal Center has recorded and published data on all intakes since October 1st 2013. That means we have 7 years worth of data! You can see the current data here.\r\nRecently I was thinking about the cat of two friends of mine - Chris & Randi. Their cat is called Mr. Doggy. This led me to wonder - what other animals are cats named after - and which is the most popular?\r\nHere are the steps I took to answer this. First, load tidyverse to do the data wrangling and plotting:\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\n\r\nI downloaded the Austin Animal Center Intake data up to 13th October 2020 and put it on my github page. You can directly import that into R from my webpage like this: \r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/jalapic/rblog/master/raw_data/Austin_Animal_Center_Intakes.csv\")\r\nhead(df)\r\n\r\n# A tibble: 6 x 12\r\n  `Animal ID` Name  DateTime MonthYear `Found Location` `Intake Type`\r\n  <chr>       <chr> <chr>    <chr>     <chr>            <chr>        \r\n1 A786884     *Bro~ 01/03/2~ 01/03/20~ 2501 Magin Mead~ Stray        \r\n2 A706918     Belle 07/05/2~ 07/05/20~ 9409 Bluegrass ~ Stray        \r\n3 A724273     Runs~ 04/14/2~ 04/14/20~ 2818 Palomino T~ Stray        \r\n4 A665644     <NA>  10/21/2~ 10/21/20~ Austin (TX)      Stray        \r\n5 A682524     Rio   06/29/2~ 06/29/20~ 800 Grove Blvd ~ Stray        \r\n6 A743852     Odin  02/18/2~ 02/18/20~ Austin (TX)      Owner Surren~\r\n# ... with 6 more variables: `Intake Condition` <chr>, `Animal\r\n#   Type` <chr>, `Sex upon Intake` <chr>, `Age upon Intake` <chr>,\r\n#   Breed <chr>, Color <chr>\r\n\r\n\r\nWe have 12 columns in total. These are as follows: \r\n\r\n\r\ncolnames(df)\r\n\r\n [1] \"Animal ID\"        \"Name\"             \"DateTime\"        \r\n [4] \"MonthYear\"        \"Found Location\"   \"Intake Type\"     \r\n [7] \"Intake Condition\" \"Animal Type\"      \"Sex upon Intake\" \r\n[10] \"Age upon Intake\"  \"Breed\"            \"Color\"           \r\n\r\n\r\nLet’s look at what animal type is: \r\n\r\n\r\ntable(df$`Animal Type`)\r\n\r\n     Bird       Cat       Dog Livestock     Other \r\n      567     45568     68650        21      6407 \r\n\r\n\r\nFor this analysis, we’ll just focus on cats:\r\n\r\n\r\n\r\ncats <- df %>% filter(`Animal Type` == \"Cat\")\r\nhead(cats)\r\n\r\n# A tibble: 6 x 12\r\n  `Animal ID` Name  DateTime MonthYear `Found Location` `Intake Type`\r\n  <chr>       <chr> <chr>    <chr>     <chr>            <chr>        \r\n1 A665644     <NA>  10/21/2~ 10/21/20~ Austin (TX)      Stray        \r\n2 A774147     <NA>  06/11/2~ 06/11/20~ 6600 Elm Creek ~ Stray        \r\n3 A731435     *Cas~ 08/08/2~ 08/08/20~ Austin (TX)      Owner Surren~\r\n4 A790209     Ziggy 03/06/2~ 03/06/20~ 4424 S Mopac Ex~ Public Assist\r\n5 A743114     <NA>  02/04/2~ 02/04/20~ 208 Beaver St i~ Stray        \r\n6 A657188     Tommy 11/10/2~ 11/10/20~ Austin (TX)      Owner Surren~\r\n# ... with 6 more variables: `Intake Condition` <chr>, `Animal\r\n#   Type` <chr>, `Sex upon Intake` <chr>, `Age upon Intake` <chr>,\r\n#   Breed <chr>, Color <chr>\r\n\r\n\r\n\r\n\r\nnrow(cats)\r\n\r\n[1] 45568\r\n\r\nnrow(cats[!is.na(cats$Name),])\r\n\r\n[1] 25181\r\n\r\nWe have 45,568 cats in the dataset, and 25,181 of these have names!\r\n\r\nTo make the data a bit more readable, let’s just keep the columns that give the name, intake type, sex, age, breed and color. I’ll also clean up the column names:\r\n\r\n\r\n\r\ncats1 <- cats %>% select(2,7,9:12)\r\ncolnames(cats1) <- c(\"name\",\"type\",\"sex\",\"age\",\"breed\",\"color\")\r\nhead(cats1)\r\n\r\n# A tibble: 6 x 6\r\n  name   type    sex         age     breed              color         \r\n  <chr>  <chr>   <chr>       <chr>   <chr>              <chr>         \r\n1 <NA>   Sick    Intact Fem~ 4 weeks Domestic Shorthai~ Calico        \r\n2 <NA>   Injured Intact Fem~ 4 weeks Domestic Shorthai~ Black/White   \r\n3 *Casey Normal  Neutered M~ 5 mont~ Domestic Shorthai~ Cream Tabby   \r\n4 Ziggy  Normal  Intact Fem~ 4 years Domestic Shorthai~ Brown Tabby/W~\r\n5 <NA>   Injured Intact Fem~ 2 years Domestic Shorthai~ Black/White   \r\n6 Tommy  Normal  Neutered M~ 14 yea~ Domestic Shorthai~ Brown Tabby/W~\r\n\r\n\r\nAs you can see, not every cat has a given name. But we can do a search for any name we want. Let’s see if there are any cats with the name “fluff” or “fluffy”. I’ll first make all the cat names lower case, and I’ll remove the asterisks that some names have:\r\n\r\n\r\n\r\ncats1$name <- tolower(gsub(\"\\\\*\", \"\", cats1$name))\r\nhead(cats1)\r\n\r\n# A tibble: 6 x 6\r\n  name  type    sex          age     breed              color         \r\n  <chr> <chr>   <chr>        <chr>   <chr>              <chr>         \r\n1 <NA>  Sick    Intact Fema~ 4 weeks Domestic Shorthai~ Calico        \r\n2 <NA>  Injured Intact Fema~ 4 weeks Domestic Shorthai~ Black/White   \r\n3 casey Normal  Neutered Ma~ 5 mont~ Domestic Shorthai~ Cream Tabby   \r\n4 ziggy Normal  Intact Fema~ 4 years Domestic Shorthai~ Brown Tabby/W~\r\n5 <NA>  Injured Intact Fema~ 2 years Domestic Shorthai~ Black/White   \r\n6 tommy Normal  Neutered Ma~ 14 yea~ Domestic Shorthai~ Brown Tabby/W~\r\n\r\n\r\nWe can use grepl() to search for partial name matches: \r\n\r\n\r\ncats1 %>% filter(grepl(\"fluf\", name))\r\n\r\n# A tibble: 52 x 6\r\n   name      type    sex         age    breed             color       \r\n   <chr>     <chr>   <chr>       <chr>  <chr>             <chr>       \r\n 1 fluffy    Normal  Intact Fem~ 3 wee~ Domestic Medium ~ Brown Tabby \r\n 2 fluffy    Normal  Spayed Fem~ 2 mon~ Domestic Shortha~ Blue Tabby  \r\n 3 fluffern~ Normal  Intact Male 4 wee~ Domestic Shortha~ Blue        \r\n 4 fluff     Normal  Intact Fem~ 3 wee~ Domestic Shortha~ Brown Tabby~\r\n 5 fluffy    Injured Neutered M~ 9 yea~ Domestic Longhai~ Black/White \r\n 6 fluff     Normal  Spayed Fem~ 2 yea~ Domestic Longhai~ Black       \r\n 7 fluff     Normal  Spayed Fem~ 5 yea~ Domestic Shortha~ Blue Tabby  \r\n 8 fluffy    Normal  Unknown     2 yea~ Domestic Longhai~ Cream       \r\n 9 fluffy    Normal  Spayed Fem~ 2 yea~ Domestic Longhai~ Black       \r\n10 fluffy    Normal  Neutered M~ 5 yea~ Domestic Medium ~ Blue Tabby  \r\n# ... with 42 more rows\r\n\r\n\r\nTurns out there are 52 different *fluf* variations - they are easier to see if we use table() on the name column:\r\n\r\n\r\n\r\ncats1 %>% filter(grepl(\"fluf\", name)) %>% .$name %>% table()\r\n\r\n.\r\n         fluff        fluffer fluffer nutter    flufferkins \r\n            19              2              1              1 \r\n   fluffernutt     flufferton      fluffette         fluffy \r\n             1              1              1             21 \r\n   fluffy butt   fluffy fluff    franz fluff    fuzzyfluffy \r\n             1              1              1              1 \r\n  mr. fluffers \r\n             1 \r\n\r\n\r\nCats that are also animals:\r\nNext, I realized that I needed a list of animals. Google helped me out and I found this list here. I just found read.csv() the quickest way of doing this: \r\n\r\n\r\nanimals <- read.csv(\"https://gist.githubusercontent.com/atduskgreg/3cf8ef48cb0d29cf151bedad81553a54/raw/82f142562cf50b0f6fb8010f890b2f934093553e/animals.txt\",\r\n                    header=F, stringsAsFactors = F)\r\n\r\nhead(animals)\r\n\r\n       V1\r\n1 Canidae\r\n2 Felidae\r\n3     Cat\r\n4  Cattle\r\n5     Dog\r\n6  Donkey\r\n\r\ntail(animals)\r\n\r\n                       V1\r\n518         Ringneck dove\r\n519                 Sheep\r\n520 Siamese fighting fish\r\n521         Society finch\r\n522                   Yak\r\n523         Water buffalo\r\n\r\n\r\nNext, we just need to filter the cats1 dataset for whether they contain the animal name. If we went for direct matches, we would be able to pull out e.g. “dog” but not “Mr dog”. But then, if we just went for partial matches, we might get some errors too. First I’ll try direct matches - i.e. the cat just has another animal’s name: \r\n\r\n\r\ncats1 %>% filter(name %in% tolower(animals[,1]))\r\n\r\n# A tibble: 545 x 6\r\n   name    type   sex        age    breed                 color       \r\n   <chr>   <chr>  <chr>      <chr>  <chr>                 <chr>       \r\n 1 sparrow Normal Neutered ~ 1 mon~ Domestic Shorthair M~ Black/White \r\n 2 harrier Normal Intact Ma~ 1 mon~ Domestic Shorthair M~ Brown Tabby \r\n 3 dolphin Normal Intact Fe~ 5 mon~ Domestic Shorthair M~ Blue        \r\n 4 cat     Normal Spayed Fe~ 6 yea~ Maine Coon Mix        Brown Tabby \r\n 5 eagle   Normal Intact Ma~ 4 wee~ American Curl Shorth~ Lynx Point  \r\n 6 deer    Normal Intact Fe~ 3 wee~ Domestic Shorthair    Tortie      \r\n 7 bear    Normal Intact Ma~ 9 mon~ Siamese/Domestic Sho~ Black       \r\n 8 kiwi    Normal Neutered ~ 1 year Domestic Shorthair M~ Brown Tabby~\r\n 9 possum  Normal Intact Fe~ 1 day  Domestic Shorthair M~ Calico      \r\n10 monkey  Normal Intact Fe~ 3 yea~ Domestic Shorthair M~ Tortie      \r\n# ... with 535 more rows\r\n\r\n\r\nLet’s get a frequency count of these names:\r\n\r\n\r\n\r\ncats1 %>% filter(name %in% tolower(animals[,1])) %>% \r\n  .$name %>% table()\r\n\r\n.\r\n  armadillo      badger        bass         bat        bear \r\n          1           6           2           1          28 \r\n        bee      beetle        bird   blackbird         bug \r\n          3           3           1           1           7 \r\n  butterfly     caribou         cat caterpillar     catfish \r\n          4           2           6           2           2 \r\n    cheetah   chickadee     chicken  chinchilla    chipmunk \r\n          4           2           7           2           1 \r\n     cicada       cobra      condor        crab     cricket \r\n          1           1           1           1          14 \r\n       crow        deer       dingo     dolphin        dove \r\n          1           2           1           2           3 \r\n  dragonfly        duck       eagle    elephant         elk \r\n          3           4           5           1           3 \r\n     ermine      falcon       finch     firefly        fish \r\n          1           4           3           4           2 \r\n        fly         fox        frog       goose      gopher \r\n          1           2           2          15           1 \r\ngrasshopper       guppy     harrier        hawk      jaguar \r\n          7           9           1           5           3 \r\n        jay    kangaroo        kiwi       koala     ladybug \r\n          8           1          14           1           6 \r\n       lark     lemming     leopard        lion     lobster \r\n          2           1           1           2           1 \r\n       lynx    mackerel      marlin        mink        mite \r\n          5           1           1           1           1 \r\n     monkey       moose       mouse     narwhal        newt \r\n         21          18          15           1           2 \r\n    opossum        orca       otter         owl       panda \r\n          4           1           8           3           9 \r\n    panther     penguin       perch         pig      pigeon \r\n         12           5           1           1           5 \r\n   platypus        pony      possum      puffin        puma \r\n          1           1           5           2          10 \r\n     rabbit     raccoon       raven     rooster    scorpion \r\n          4           1          19           4           1 \r\n     shrimp       skunk       snipe     sparrow      spider \r\n          2           2           1          10           3 \r\n      squid    squirrel    sturgeon        swan       swift \r\n          5           6           1           2           1 \r\n      tiger        toad    tortoise       trout        tuna \r\n         68           3           1           2          13 \r\n     turkey      turtle     wallaby      weasel        wolf \r\n         10           9           1           2           1 \r\n  wolverine      wombat        wren       zebra \r\n          2           3           7           2 \r\n\r\n Looks like there is no dog !\r\nHere are the top 10: \r\n\r\n\r\ncats1 %>% filter(name %in% tolower(animals[,1])) %>% \r\n  group_by(name) %>% count() %>% arrange(-n)\r\n\r\n# A tibble: 114 x 2\r\n# Groups:   name [114]\r\n   name        n\r\n   <chr>   <int>\r\n 1 tiger      68\r\n 2 bear       28\r\n 3 monkey     21\r\n 4 raven      19\r\n 5 moose      18\r\n 6 goose      15\r\n 7 mouse      15\r\n 8 cricket    14\r\n 9 kiwi       14\r\n10 tuna       13\r\n# ... with 104 more rows\r\n\r\n\r\nLet’s make a plot of any animal that has more than 5 instances:\r\n\r\n\r\n\r\ncats1 %>% filter(name %in% tolower(animals[,1])) %>% \r\n  group_by(name) %>% count() %>% arrange(-n) %>%\r\n  filter(n>5) %>%\r\n ggplot( aes(x = reorder(name, n), y = n) ) + \r\n  geom_col(fill = \"#123abc\", color=\"#193642\", alpha=.4) +\r\n  xlab(\"\") +\r\n  ylab(\"Total Cats\") +\r\n  ggtitle(\"Cats as Animals\") +\r\n  theme_classic() +\r\n  coord_flip()\r\n\r\n\r\n\r\nThat will do for now! There’s much more we can do with this dataset in the future. Although, I will do a quick, final check for “Mr Dog” or “Mr Doggie” \r\n\r\n\r\ncats1 %>% filter(grepl(\"dog\", name))\r\n\r\n# A tibble: 1 x 6\r\n  name   type   sex           age     breed                 color     \r\n  <chr>  <chr>  <chr>         <chr>   <chr>                 <chr>     \r\n1 hotdog Normal Spayed Female 2 years Domestic Longhair Mix Blue Tabby\r\n\r\n Nope - just ‘hotdog’ - a two year old blue tabby.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-13-cat-names/cat-names_files/figure-html5/unnamed-chunk-15-1.png",
    "last_modified": "2021-10-03T13:41:28-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to this blog, James' R Blog. I hope you enjoy \nreading it!",
    "author": [
      {
        "name": "James Curley",
        "url": "https://jamescurley.blog"
      }
    ],
    "date": "2020-10-13",
    "categories": [],
    "contents": "\r\nWelcome to this blog. I’m an Associate Professor in Psychology at UT Austin. My plan is to put here bits and pieces of R code and data that I find fun or useful. They are probably things that I want to play around with that aren’t directly related to my research, or stuff that students might like to work on.\r\nGet in touch with me at curley AT utexas DOT edu\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-10-03T13:41:28-05:00",
    "input_file": {}
  }
]
